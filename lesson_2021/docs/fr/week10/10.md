---
lang: fr
lang-ref: ch.10
title: Semaine 10
translation-date: 19 June 2021
translator: Loïck Bourdois
---


<!--
## Lecture part A
A brief introduction to self-supervised learning and pretext tasks and discussion of associated trivial solutions. Categorization of recent self-supervised methods: Introduction to Contrastive Learning and the loss function used. Brief overviews of PIRL, SimCLR and MoCo followed by SwAV which is a Clustering based method. Pretraining on Imagenet and non-Imagenet data is also discussed towards the end.
-->
## Cours magistral partie A

Une brève introduction à l'apprentissage autosupervisé et aux tâches de prétexte ainsi qu'une discussion à propos des solutions triviales associées. Puis une catégorisation des méthodes autosupervisées récentes avec une introduction à l'apprentissage contrastif et à la fonction de perte utilisée. Nous poursuivons avec de brèves présentations de PIRL, SimCLR et MoCo suivies de SwAV qui est une méthode basée sur du *clustering*. Le pré-entraînement sur les données ImageNet et non-ImageNet est également discuté à la fin.

<!--
## Lecture part B
-->
## Cours magistral partie B

<!--
## Practicum
We introduce attention, focusing on self-attention and its hidden layer representations of the inputs. Then, we introduce the key-value store paradigm and discuss how to represent queries, keys, and values as rotations of an input. Finally, we use attention to interpret the transformer architecture taking a forward pass through a basic transformer through an EBM perspective,, and comparing the encoder-predictor-decoder paradigm to sequential architectures.
-->
## Travaux dirigés

Nous présentons l'attention en nous concentrant sur l'auto-attention et ses représentations des entrées dans la couche cachée. Ensuite, nous introduisons le paradigme clé-valeur et discutons de la manière de représenter les requêtes, les clés et les valeurs comme des rotations d'une entrée. Enfin, nous utilisons l'attention pour interpréter l'architecture du *transformer*. Pour cela nous passons par le biais d'un *transformer* de base dans la perspective des EBMs et en comparant le paradigme encodeur-prédicteur-décodeur aux architectures séquentielles.
