---
lang: fr
lang-ref: ch.10-2
title: SEER, AVID + CMA, Distillation, Barlow Twins
lecturer: Ishan Misra
authors: Duc Anh Phi, Krishna Karthik Reddy Jonnala
date: 17 May 2021
typora-root-url: 10-2
translation-date: 21 Jun 2021
translator: Loïck Bourdois
---

<!--
## SEER: Learning from uncharted Images
Compared to Imagenet dataset, real world images may have different distributions (cartoons, memes) and may or may not have a prominent object. In order to verify if the models work well on images outside of Imagenet dataset we decided to test *Swav* method on large scale data. SEER is *Swav* method tested on billions of unfiltered images.

Following graph compares the fine tune performance of the four models when transfered to Imagenet. Using SEER method, a model can be trained with more than a billion parameters which are going to transfer really well to Imagenet.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/seer_1.png" style="background-color:#DCDCDC;" /><br>
Figure 1 Comparing SEER to other methods on ImageNet data
</center>

As shown in the following table, the performance of SEER is comparable to the networks trained on curated data with weak supervision.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/seer_2.png" style="background-color:#DCDCDC;" /><br>
Figure 2 SEER performance vs weak supervision model
</center>
-->

## SEER : Apprendre à partir d'images non répertoriées
Par rapport au jeu de données Imagenet, les images du monde réel peuvent avoir des distributions différentes (dessins animés, memes) et avoir ou non un objet proéminent. 
Afin de vérifier si les modèles fonctionnent bien sur des images en dehors de la base de données Imagenet, les auteurs de SEER ont décidé de tester la méthode SWAV sur des données à grande échelle.
SEER (*SElf-supERvised*) est donc la méthode SWAV testée sur des milliards d'images non filtrées.

Le graphique suivant compare les performances des quatre modèles *finetunés* sur Imagenet. 
En utilisant la méthode SEER, un modèle peut être entraîné avec plus d'un milliard de paramètres qui vont se transférer très bien à Imagenet.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/seer_1.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 1 : </b> Comparaison sur ImageNet de SEER à d'autres méthodes 
</center>

Comme le montre le tableau suivant, les performances de SEER sont comparables à celles des réseaux entraînés sur des données organisées avec une supervision faible.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/seer_2.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 2 : </b> Performance de SEER par rapport au modèle de supervision faible
</center>


<!--
## AVID + CMA
Audio Visual Instance Discrimination with Cross Modal Agreement is a method that combines *contrastive learning* and *clustering* techniques.

For contrastive leaning on an Audio-Video dataset, when the (audio-video) inputs are passed to the two encoders ($f_a, f_v$) we will get two embeddings (audio and video). The embeddings from the same sample should be close in feature space compared to embeddings from different samples.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/avid.png" style="background-color:#DCDCDC;" /><br>
Figure 3 AVID: Audio Video Instance Discrimination
</center>

To introduce the *clustering*, the notion of the positives and negatives is expanded as shown in the following image. Computing the similarities in the video and audio embeddings from a reference point to all the other samples results in *Positive Set* and *Negative Set*. A sample falls into positive set when both its audio and video embeddings are similar to the reference embeddings.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/cma.png" style="background-color:#DCDCDC;" /><br>
Figure 4 CMA: Cross-Modal Agreements
</center>
-->

## AVID + CMA
*Audio Visual Instance Discrimination with Cross Modal Agreement* est une méthode qui combine l'apprentissage contrastif et les techniques de *clustering*.

Pour l'apprentissage contrastif sur un jeu de données audio-vidéo, lorsque les entrées (audio-vidéo) sont transmises aux deux encodeurs ($f_a, f_v$), nous obtenons deux enchâssements (audio et vidéo). 
Les enchâssements d'un même échantillon doivent être proches dans l'espace des caractéristiques par rapport aux enchâssements de différents échantillons.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/avid.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 3 : </b> AVID - Audio Visual Instance Discrimination
</center>

Pour introduire le *clustering*, la notion de *positifs* et *négatifs* est étendue comme le montre l'image suivante. 
Le calcul des similarités dans les enchâssements vidéo et audio à partir d'un point de référence vers tous les autres échantillons donne lieu à un *ensemble positif* et un *ensemble négatif*. 
Un échantillon tombe dans l'ensemble positif lorsque ses enchâssements audio et vidéo sont tous deux similaires aux enchâssements de référence.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/cma.png" style="background-color:#DCDCDC;" /><br>
  <b>Figure 4 :</b>  <i>Cross Modal Agreement</i> (CMA)
</center>


<!--
## Distillation
Distillation methods are similarity maximization based methods. Like other SSL methods distillation tries to prevent trivial solutions. It does so by asymmetry in two different ways.
* Asymmetric *learning rule* between student teacher
* Asymmetric *architecture* between student teacher

$$ f_{\vtheta}^{\text{student}}(I) = f_{\vtheta}^{\text{teacher}}(\text{augment}(I))$$

### BYOL
BYOL is a distillation technique whose architecture is shown below.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/byol.png" style="background-color:#DCDCDC;" /><br>
Figure 5 BYOL architecture
</center>

There is an asymmetry in architecture between student teacher as student has an additional prediction head. The gradient backpropagation only happens through Student encoder clearly creating an asymmetry in learning rate. In BYOL there is an additional source of asymmetry which is in weights of student encoder and teacher encoder. Teacher encoder is created as moving average of student encoder. These asymmetries will prevent the model from trivial solutions.

### SimSiam
Recent studies showed that all the three sources of asymmetry discussed in BYOL are not needed to prevent the trivial solutions. In *SimSiam* architecture the student and teacher share the same set of weights and there are two sources of asymmetry.
* In architecture of student encoder with an additional predictor head.
* In learning rate, when backpropagating the gradients are passed only through student encoder but not the teacher encoder. After each epoch, the weights of student encoder are copied to the teacher encoder.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/simsiam.png" style="background-color:#DCDCDC;" /><br>
Figure 6 SimSiam architecture
</center>
-->


## Distillation
Les méthodes de distillation sont des méthodes basées sur la maximisation de la similarité.
Comme les autres méthodes d'apprentissage autosupervisé, la distillation tente d'empêcher les solutions triviales. Elle le fait par l'asymétrie de deux manières différentes :
1. Règle d'apprentissage asymétrique entre l'étudiant et l'enseignant.
2. L'architecture asymétrique entre l'étudiant et l'enseignant.

$$ f_{\vtheta}^{\text{student}}(I) = f_{\vtheta}^{\text{teacher}}(\text{augment}(I))$$

### BYOL
BYOL (*Bootstrap Your Own Latent*) est une technique de distillation dont l'architecture est présentée ci-dessous.
<center>
<img src="{{site.baseurl}}/images/week10/10-2/byol.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 5 : </b> Architecture BYOL
</center>

Il y a une asymétrie dans l'architecture entre l'étudiant et l'enseignant car l'étudiant a une tête de prédiction supplémentaire. 
La rétropropagation du gradient ne se fait qu'à travers l'encodeur de l'étudiant créant clairement une asymétrie dans le taux d'apprentissage.
Dans BYOL, il y a une source supplémentaire d'asymétrie qui se trouve dans les poids de l'encodeur de l'étudiant et de l'encodeur de l'enseignant avec l'encodeur de l'enseignant qui est créé comme une moyenne mobile de l'encodeur de l'étudiant. Ces trois asymétries vont empêcher le modèle d'avoir des solutions triviales.

### SimSiam
Des études récentes ont montré que les trois sources d'asymétrie présentes dans BYOL ne sont pas nécessaires pour empêcher les solutions triviales. 
Dans l'architecture SimSiam (*Simple Siamese*), l'étudiant et l'enseignant partagent le même ensemble de poids et il existe deux sources d'asymétrie :
1. Dans l'architecture de l'encodeur de l'étudiant avec une tête de prédiction supplémentaire.
2. Dans le taux d'apprentissage, lors de la rétropropagation, où les gradients ne passent que par l'encodeur de l'étudiant mais pas par celui de l'enseignant. Après chaque époque, les poids de l'encodeur étudiant sont copiés dans l'encodeur de l'enseignant.

<center>
<img src="{{site.baseurl}}/images/week10/10-2/simsiam.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 6 :</b> Architecture de SimSiam
</center>


<!--
## Barlow Twins

### Hypothesis from information theory
The efficient coding hypothesis was proposed by Horace Barlow in 1961 as a theoretical model of sensory coding in the brain. Within the brain, neurons communicate with each other by sending electrical impulses called spikes. Barlow hypothesised that the spikes in the sensory system form a neural code for efficiently representing sensory information. By efficient, Barlow meant that the code minimises the number of spikes needed to transmit a given signal. 

### Implementation
A successful approach to Self-Supervised-Learning (SSL) is to learn representations which are invariant to distortions of the input sample. However, a recurring problem with this approach is the existence of trivial constant solutions.

The Barlow Twins method proposes an objective function that naturally avoids such collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample and making them as close as possible to the identity matrix.

Barlow's redundancy-reduction principle applied to a pair of identical networks. The objective function measures the cross-correlation matrix between the output features of two identical networks fed with distorted versions of a batch of samples and attemps to bring this matrix close to the identity. This causes the representation vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors (Figure 7).

<center>
<img src="{{site.baseurl}}/images/week10/10-2/figure_1.png" style="background-color:#DCDCDC;" /><br>
Figure 7 Barlow-Twins Architecture
</center>

More formally, it produces two distorted views for all images of a batch $X$. The distorted views are obtained via a distribution of data augmentations $\mathcal{T}$. The two batches of distorted views $Y^A$ and $Y^B$ are then fed to a function $f_{\vtheta}$, typically a deep network with trainable parameters $\vtheta$, producing batches of representations $Z^{A}$ and $Z^{B}$ respectively. 

The loss function $\mathcal{L_{BT}}$ contains a invariance and redundancy reduction:

$$
\mathcal{L_{BT}} \triangleq  \underbrace{\sum_i  (1-\mathcal{C}_{ii})^2}_\text{invariance term}  + ~~\lambda \underbrace{\sum_{i}\sum_{j \neq i} {\mathcal{C}_{ij}}^2}_\text{redundancy reduction term}
$$

where $\lambda$ is a constant controlling the importance of the first and second terms of the loss, and where $\mathcal{C}$ is the cross-correlation matrix computed between the outputs of the two identical networks along the batch dimension:

$$
\mathcal{C}_{ij} \triangleq \frac{
\sum_b z^A_{b,i} z^B_{b,j}}
{\sqrt{\sum_b {(z^A_{b,i})}^2} \sqrt{\sum_b {(z^B_{b,j})}^2}}
$$

where $b$ indexes batch samples and $i,j$ index the vector dimension of the networks' outputs. $\mathcal{C}$ is a square matrix with size the dimensionality of the network's output. In other words 

Intuitively, the invariance term of the objective, by trying to equate the diagonal elements of the cross-correlation matrix to 1, makes the representation invariant to the distortions applied.  The redundancy reduction term, by trying to equate the off-diagonal elements of the cross-correlation matrix to 0, decorrelates the different vector components of the representation. This decorrelation reduces the redundancy between output units, so that the output units contain non-redundant information about the sample. 
-->

## *Barlow Twins*

### Hypothèse issue de la théorie de l'information
L'hypothèse du codage efficace a été proposée par Horace Barlow en 1961 comme modèle théorique du codage sensoriel dans le cerveau. 
Barlow a émis l'hypothèse que les impulsions électriques dans le cerveau forment un code neuronal permettant de représenter efficacement les informations sensorielles. C'est-à-dire que ce code minimise le nombre d'impulsions nécessaires pour transmettre un signal donné notamment d'un point de vue énergétique notamment. Cette hypothèse est basée sur la théorie de l'information. 


### Mise en œuvre
La méthode des *Barlow Twins* propose une fonction objectif qui évite naturellement l'effondrement causé par les solutions triviales. Elle mesure la matrice de corrélation croisée entre les sorties des deux réseaux identiques (ayant préalablement eu en entrée des versions déformées d'un échantillon) et en les rendant aussi proches que possible de la matrice d'identité.

Les vecteurs de représentation des versions déformées d'un échantillon sont ainsi similaires, tout en minimisant la redondance entre les composantes de ces vecteurs (figure 7).

<center>
<img src="{{site.baseurl}}/images/week10/10-2/figure_1.png" style="background-color:#DCDCDC ;" /><br>
  <b>Figure 7 :</b> Architecture des <i>Barlow Twins</i>
</center>

Plus formellement, cela produit deux vues déformées pour toutes les images d'un batch $X$. 
Les vues déformées sont obtenues via une distribution d'augmentations de données $\mathcal{T}$. 
Les deux batchs de vues déformées $Y^A$ et $Y^B$ sont ensuite soumis à une fonction $f_{\vtheta}$, typiquement un réseau profond avec des paramètres entraînables $\vtheta$, produisant  respectivement des batchs de représentations $Z^{A}$ et $Z^{B}$. 

La fonction de perte $\mathcal{L_{BT}}$ contient un terme d'invariance et un terme de réduction de la redondance :

$$
\mathcal{L_{BT}} \triangleq  \underbrace{\sum_i  (1-\mathcal{C}_{ii})^2}_\text{terme d'invariance}  + ~~\lambda \underbrace{\sum_{i}\sum_{j \neq i} {\mathcal{C}_{ij}}^2}_\text{terme de réduction de la redondance}
$$

où $\lambda$ est une constante contrôlant l'importance des premier et deuxième termes de la perte.
Et on a $\mathcal{C}$ la matrice de corrélation croisée calculée entre les sorties des deux réseaux identiques le long de la dimension du batch :

$$
\mathcal{C}_{ij} \triangleq \frac{
\sum_b z^A_{b,i} z^B_{b,j}}
{\sqrt{\sum_b {(z^A_{b,i})}^2} \sqrt{\sum_b {(z^B_{b,j})}^2}}
$$

où $b$ indexe les échantillons de batch et $i,j$ indexe la dimension vectorielle des sorties des réseaux. 
$\mathcal{C}$ est une matrice carrée dont la taille correspond à la dimension de la sortie du réseau.<br>

Intuitivement, en essayant d'égaliser les éléments diagonaux de la matrice de corrélation croisée à $1$, le terme d'invariance de l'objectif rend la représentation invariante aux distorsions appliquées.  
En essayant d'égaliser les éléments hors diagonale de la matrice de corrélation croisée à $0$, le terme de réduction de la redondance décorréle les différentes composantes vectorielles de la représentation. 
Cette décorrélation réduit la redondance entre les unités de sortie, de sorte que les unités de sortie contiennent des informations non redondantes sur l'échantillon.
