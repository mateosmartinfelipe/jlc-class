---
lang: fr
lang-ref: ch.10-3
lecturer: Alfredo Canziani
title: Architecture encodeur-predicteur-décodeur d'un transformer
authors: Rahul Ahuja, jingshuai jiang
date: 15 Apr 2021
typora-root-url: 10-3
translation-date: 20 Jun 2021
translator: Loïck Bourdois
---       


<!--
## The Transformer

Before elaborating the encoder-predictor-decoder architecture, we are going to review two models we've seen before.
-->
## Le *transformer*

Avant d'élaborer l'architecture encodeur-prédicteur-décodeur, nous allons passer en revue deux modèles que nous avons déjà vus.


<!--
### Conditional EBM latent variable architecture

We should be familiar with the terminology of these modules from the previous lectures.
In the conditional EBM latent variable architecture, we have $x$ the conditional variable which goes into a predictor. We have $\vy$ which is the target value. The decoder modules will produce $\vytilde$ when fed with a latent variable $z$ and the output of the predictor. $\red{E}$ is the energy function which minimizes the energy between $\vytilde$ and $\vy$.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/ebm.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 1: </b> (From the EBM lecture) Diagram above depicting the architecture of a conditional EBM latent variable model.
</center>
-->

### Architecture d'un EBM conditionnel à variable latente

Dans l'architecture d'un EBM conditionnel à variable latente, nous avons $x$ la variable conditionnelle qui va dans un prédicteur.
Nous avons $\vy$ qui est la valeur cible. Les modules de décodage produisent $\vytilde$ lorsqu'on leur donne une variable latente $z$ et la sortie du prédicteur. 
$\red{E}$ est la fonction d'énergie qui minimise l'énergie entre $\vytilde$ et $\vy$.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/autoencoder.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 1 : </b> Architecture d'un EBM conditionnel à variable latente
</center>


<!--
### Autoencoder architecture

In Autoencoder architecture , we observed there is no conditional input but only a target variable. The entire architecture is trying to learn the structure in these target variables. The target value $\vy$ is fed through an encoder module which transforms into a hidden representation space, forcing only the most important information through. And the decoder will make these variables come back to the original target space with a $\vytilde$. And the cost function will try to minimize the distance between $\vytilde$ and $\vy$.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/autoencoder.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 2: </b> (From the autoencoder lecture) Architecture of a basic Autoencoder consisting of encoder and decoder modules.
</center>
-->

### Architecture d'un auto-encodeur

Dans l'architecture d'un auto-encodeur, nous avons observé qu'il n'y a pas d'entrée conditionnelle mais seulement une variable cible.
L'architecture entière essaie d'apprendre la structure de ces variables cibles. 
La valeur cible $\vy$ est introduite dans un module encodeur qui la transforme en un espace de représentation caché, ne laissant passer que les informations les plus importantes. 
Et le décodeur fera en sorte que ces variables reviennent à l'espace cible original avec une valeur $\vytilde$.
La fonction de coût va essayer de minimiser la distance entre $\vytilde$ et $\vy$.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/autoencoder.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 2 : </b> Architecture d'un autoencodeur de base composé de modules encodeur et décodeur
</center>


<!--
### Encoder-predictor-decoder architecture

<center>
<img src="{{site.baseurl}}/images/week10/10-3/transformer.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 3: </b> The transformer architecture with a unit delay module.
</center>


In a transformer, $\vy$ (target sentence) is a discrete time signal. It has discrete representation in a time index. The $\vy$ is fed into a unit delay module succeeded by an encoder. The unit delay here transforms $\vy[j] \mapsto \vy[j-1]$. The only difference with the autoencoder here is this delayed variable. So we can use this structure in the language model to produce the future when given the past.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/unit_delay.png" style="zoom: 50%; background-color:#DCDCDC;" /><br>
<b>Figure 4: </b> A unit delay module transforms $\vy[j] \mapsto \vy[j-1]$
</center>

The observed signal, $\vx$ (source sentence) , is also fed through an encoder. The output of both encoder and delayed encoder are fed into the predictor, which gives a hidden representation $\vh$. This is very similar to denoising autoencoder as the delay module acts as noise in this case. And $\vx$ here makes this entire architecture a conditional delayed denoising autoencoder.
-->

### Architecture de l'encodeur-prédicteur-décodeur

<center>
<img src="{{site.baseurl}}/images/week10/10-3/transformer.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
  <b>Figure 3 : </b> L'architecture du <i>transformer</i> avec un module de retard unitaire
</center>


Dans un *transformer*, $\vy$ (phrase cible) est un signal temporel discret. Il a une représentation discrète dans un index temporel. 
Le $\vy$ est introduit dans un module de retard unitaire suivi d'un encodeur. Le retard unitaire transforme ici $\vy[j] \mapsto \vy[j-1]$ 
La seule différence avec l'auto-encodeur ici est cette variable retardée. 
Nous pouvons donc utiliser cette structure dans le modèle de langage pour produire le futur lorsqu'on nous donne le passé.

<center>
<img src="{{site.baseurl}}/images/week10/10-3/unit_delay.png" style="zoom: 50%; background-color:#DCDCDC;" /><br>
<b>Figure 4 : </b> Un module de retard unitaire transforme $\vy[j] \mapsto \vy[j-1]$
</center>

Le signal observé, $\vx$ (phrase source) passe également par un encodeur.
La sortie de l'encodeur et de l'encodeur retardé est introduite dans le prédicteur qui donne une représentation cachée $\vh$. 
Ceci est très similaire à l'auto-encodeur débruiteur car le module de retard agit comme un bruit dans ce cas.
$\vx$ fait de cette architecture entière un auto-encodeur débruiteur conditionnel retardé.


<!--
### Encoder module
You can see the detailed explaination of these modules from last year's slides [here](https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/).
-->

### Module encodeur
Vous pouvez voir l'explication détaillée de ce module dans les notes de l'année dernière disponibles [ici](https://atcold.github.io/pytorch-Deep-Learning/fr/week12/12-3/).


<!--
### Predictor Module

The transformer predictor module follows a similar procedure as the encoder. However, there is one additional sub-block (i.e. cross-attention) to take into account. Additionally, the output of the encoder modules acts as the inputs to this module.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/predictor.png" style="zoom: 100%; background-color:#DCDCDC;" /><br>
<b>Figure 5: </b> The predictor module consisting of a cross attention block
</center>
-->

### Module prédicteur

Le module prédicteur du *transformer* suit une procédure similaire à celle de l'encodeur.
Cependant, il y a un sous-bloc supplémentaire (c'est-à-dire l'attention croisée) à prendre en compte.
De plus, la sortie des modules encodeurs agit comme les entrées de ce module.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/predictor.png" style="zoom: 100%; background-color:#DCDCDC;" /><br>
<b>Figure 5 : </b> Le module prédicteur composé d'un bloc d'attention croisée
</center>


<!--
### Cross attention
You can see the detailed explaination of cross attention from last year's slides [cross-attention](https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/).
-->

### Attention croisée
Vous pouvez consulter l'explication détaillée de l'attention croisée dans les notes de l'année dernière disponibles [ici](https://atcold.github.io/pytorch-Deep-Learning/fr/week12/12-3/).

<!--
### Decoder module

Contrary to what authors of the Transformer paper define, the decoder module consists of `1D-convolution` and `Add, Norm` blocks. The output of the predictor module is fed to the decoder module and the output of the decoder module is the predicted sentence. We can train this by providing the delayed target sequence.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/decoder.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 6: </b> The correct notation of the encoder,predictor and decoder modules in a transformer
</center>
-->


### Module décodeur

Contrairement à ce que les auteurs du papier du *transformer* définissent, le module décodeur est composé de blocs `1D-convolution` et `Add, Norm`. 
La sortie du module prédicteur est introduite dans le module décodeur et la sortie du module décodeur est la phrase prédite. 
On peut l'entraîner en fournissant la séquence cible retardée.


<center>
<img src="{{site.baseurl}}/images/week10/10-3/decoder.png" style="zoom: 80%; background-color:#DCDCDC;" /><br>
<b>Figure 6 : </b> La notation correcte des modules encodeur, prédicteur et décodeur dans un <i>transformer</i>
</center>

