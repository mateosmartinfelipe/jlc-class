---
lang: fr
lang-ref: ch.10-1
title: Apprentissage autosupervisé en vision par ordinateur
lecturer: Ishan Mishra
authors: Srishti Bhargava, Jude Naveen Raj Ilango
date: 16 May 2021
typora-root-url: 10-1
translation-date: 21 Jun 2021
translator: Loïck Bourdois
---


<!--
The current focus in Computer Vision lies in learning of visual representation from supervised data and using these representations/model weights as initializations for other tasks which lack availability of labelled data. Labelling data can be expensive, for instance, the Imagenet dataset has ~14 million images and 22,000 categories taking ~22 human years to label.

### Pretext Task 

The pretext task is a self-supervised learning task solved to learn visual representations, with the aim of using the learned representations or model weights obtained in the process, for other downstream tasks. The pretext task is usually performed on a property that is inherent in the dataset itself.

Popular pretext tasks for images:
1. Predicting relative position of patches within an image
2. Predicting permutation type of image patches (Jigsaw Puzzles)
3. Predicting the kind of rotation applied

**What is missing from pretext tasks?**
 
There is a fairly big mismatch between what is being solved in the pretext tasks and what needs to be achieved by the transfer tasks. Thus, the pre-training is not suitable for final tasks. Performance at each layer can be assessed (using linear classifiers) to figure out which layer's features to use for the transfer tasks. 

Pre-trained features should satisfy two fundamental properties:
1. Must represent how images relate to one another
2. Be robust to nuisance factors
-->


La vision par ordinateur se concentre actuellement sur l'apprentissage de représentations visuelles à partir de données supervisées et sur l'utilisation de ces représentations/poids comme initialisations pour d'autres tâches qui ne disposent pas de données étiquetées. 
Cet étiquetage des données peut s'avérer coûteux. Par exemple, le jeu de données ImageNet compte environ $14$ millions d'images et $22 000$ catégories et son étiquetage a nécessité environ $22$ années de travail.

### Tâche de prétexte 

La tâche de prétexte est une tâche d'apprentissage autosupervisée effectuée dans le but d'apprendre des représentations visuelles afin de les utiliser pour d'autres tâches en aval (c'est-à-dire la classification, la détection d'objets, etc.). 
La tâche de prétexte est généralement effectuée sur une propriété inhérente au jeu de données lui-même.

Tâches de prétextes populaires pour les images :
1. Prédire la position relative des patchs dans une image.
2. Prédire le type de permutation des patchs de l'image (puzzles).
3. Prédire le type de rotation appliquée


**Qu'est-ce qui manque aux tâches de prétextes ?**
 
Il y a un décalage assez important entre ce qui est résolu avec les tâches de prétextes et ce qui doit être réalisé par les tâches de transfert. 
Ainsi, le pré-entraînement n'est pas adaptée aux tâches finales. 
Les performances de chaque couche peuvent être évaluées (à l'aide de classifieurs linéaires) afin de déterminer les caractéristiques de la couche à utiliser pour les tâches de transfert. 

Les caractéristiques pré-entraînées doivent satisfaire deux propriétés fondamentales :
1. elles doivent représenter la façon dont les images sont liées les unes aux autres
2. être robuste aux facteurs de nuisance


<!--
### Popular method for self-supervised learning
A popular and common method for self supervised learning is to learn features that are robust to data augmentation. 
Learn features such that, 

<div>
$$
\begin{aligned}
    f_{\vtheta}(I) = f_{\vtheta}(\text{augment}(I))
\end{aligned}
$$
</div>

Features produced by the network for an image should be stable under different types of data augmentation techniques. 
This property is extremely useful because the features are going to be invariant to the nuisance factors.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/ssl_trivial.png" style="width: 350px; background-color:#DCDCDC;" /><br>
Figure 1: A trivial solution to learning self-supervised representations
</center>


Take an image, apply two different data augmentations to it, feed it through encoder, compute the similarity. Get the gradients and backpropagate. 
The network then learns to produce a constant representation for both augmentations.

Trivial Solution: Produce the same features for all input images. The representations collapse and become unusable for downstream recognition tasks.
-->


### Méthode populaire pour l'apprentissage autosupervisé
Une méthode populaire et commune en apprentissage autosupervisé consiste à apprendre des caractéristiques qui sont robustes à l'augmentation de données. 
C'est à dire apprendre des caractéristiques telles que :
<div>
$$
\begin{aligned}
    f_{\vtheta}(I) = f_{\vtheta}(\text{augment}(I))
\end{aligned}
$$
</div>

Les caractéristiques produites par le réseau pour une image doivent être stables sous différents types de techniques d'augmentation de données. 
Cette propriété est extrêmement utile car les caractéristiques vont être invariantes aux facteurs de nuisance.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/ssl_trivial.png" style="width: 350px; background-color:#DCDCDC;" /><br>
<b>Figure 1 :</b> Une solution triviale pour l'apprentissage de représentations autosupervisées
</center>


Prenez une image, appliquez-lui deux augmentations de données différentes, faites-la passer par un encodeur, calculez la similarité. 
Puis faites une rétropropagation avec les gradients obtenus. Le réseau apprend alors à produire une représentation constante pour les deux augmentations.

Le problème est qu'il existe une solution triviale consistant à produire les mêmes caractéristiques pour toutes les images d'entrée. 
Les représentations s'effondrent alors et deviennent inutilisables pour les tâches de reconnaissance en aval.  
Toutes les architectures autosupervisées abordées dans la suite essaient donc d'obtenir des performances semblables à l'approche supervisée tout en évitant les solutions triviales. Il est d'ailleurs possible de les classer par famille en fonction de la manière dont chacune d'elles traite ce problème de solutions triviales.


<!--
### Categorization of recent self-supervised methods:

1. Maximize Similarity Objective
2. Redundancy Reduction Objective

For evaluation, a subset of the Imagenet dataset (1.3 million images over 1000 categories) without labels is used to pre-train a randomly initialized Resnet-50 model.

**What are the different ways to perform transfer learning and evaluate its performance?**

We can do one of 2 things:
1. Linear classifier on top of fixed features output by the network (For classification tasks)
2. Finetune the entire pretrained network on the target task (For detection tasks)

## Contrastive Learning 

**Loss function:** Embeddings from related images should be closer than embeddings from unrelated images.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/con_learning.png" style="width: 400px; background-color:#DCDCDC;" /><br>
Figure 2: The use of a base encoder in Contrastive Learning
</center>

Different Contrastive Learning methods include:

1. Pretext-Invariant Representation Learning (PIRL) 
2. SimCLR
3. MoCo
-->


### Catégorisation des méthodes autosupervisées récentes :

1. Objectif de maximisation de la similarité
2. Objectif de réduction de la redondance

Pour l'évaluation, un sous-ensemble d'ImageNet $(1,3$ million d'images dans $1000$ catégories) non étiqueté est utilisé pour pré-entraîner un modèle Resnet-50 initialisé de manière aléatoire.

**Quelles sont les différentes façons de réaliser l'apprentissage par transfert et d'évaluer ses performances?**

Nous pouvons faire l'une des deux choses suivantes :
1. Un classifieur linéaire sur les caractéristiques fixes produites par le réseau (pour les tâches de classification).
2. *Finetuner* l'ensemble du réseau pré-entraîné sur la tâche cible (pour les tâches de détection).


## Apprentissage contrastif 

**Fonction de perte :** les enchâssements d'images liées doivent être plus proches que les enchâssements d'images non liées.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/con_learning.png" style="width: 400px; background-color:#DCDCDC;" /><br>
<b>Figure 2 :</b> l'utilisation d'un encodeur de base dans l'apprentissage contrastif
</center>

Les différentes méthodes d'apprentissage contrastif comprennent :

1. PIRL (*Pretext-Invariant Representation Learning*)
2. SimCLR (*Simple Framework for Contrastive Learning of Visual Representations*)
3. MoCo (*Momentum Contrast*)


<!--
## Pretext-Invariant Representation Learning (PIRL) 

<center>
<img src="{{site.baseurl}}/images/week10/10-1/pirl.png" style="width: 400px; background-color:#DCDCDC;" /><br>
Figure 3: Architecture of PIRL
</center>

An image $I$ and its augmentation $I^t$ is passed through the network and the contrastive learning loss is applied in such a way that the network becomes invariant to the pretext task.
The aim is to achieve high similarity for image and patch features belonging to the same image. And low similarity for features from other random images. The pretext task of PIRL tries to achieve invariance over data augmentations rather than predicting the data augmentation.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/cl_loss_fn.png" style="width: 400px; background-color:#DCDCDC;" /><br>
Figure 4: Loss function of PIRL
</center>
-->

## PIRL

<center>
<img src="{{site.baseurl}}/images/week10/10-1/pirl.png" style="width: 400px; background-color:#DCDCDC;" /><br>
<b>Figure 3 :</b> Architecture de PIRL
</center>

Une image $I$ et son augmentation $I^t$ sont passées à travers le réseau et la perte d'apprentissage contrastive est appliquée de manière à ce que le réseau devienne invariant à la tâche de prétexte.
L'objectif est d'obtenir une similarité élevée pour les caractéristiques de l'image et du patch appartenant à la même image ainsi qu'une faible similarité pour les caractéristiques provenant d'autres images aléatoires. 
La tâche prétexte de PIRL essaie d'obtenir une invariance sur les augmentations de données plutôt que de prédire l'augmentation des données.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/cl_loss_fn.png" style="width: 400px; background-color:#DCDCDC;" /><br>
<b>Figure 4 :</b> Fonction de perte de PIRL
</center>

<!--
### Semantic Features
<center>
<div style="margin-left: 40px;"><img src="{{site.baseurl}}/images/week10/10-1/semantic_features.png" style="width: 350px; background-color:#DCDCDC;" /><br></div>
Figure 5: Performance of pretraining using PIRL vs Jigsaw at each layer
</center>

- A linear classifier is used at each layer to probe and compute the accuracy. One is trained using PIRL and other using Jigsaw.
- Jigsaw is trying to predict the permutation, whereas PIRL is trying to be invariant to it. 
- Performance for PIRL keeps increasing indicating that the feature is becoming increasingly aligned to the downstream classification tasks. 
- Jigsaw is trying to retain pretext information, the performance plateaues and then drops down sharply and it ends up not performing well for the transfer tasks. 
-->

### Caractéristiques sémantiques
<center>
<div style="margin-left: 40px;"><img src="{{site.baseurl}}/images/week10/10-1/semantic_features.png" style="width: 350px; background-color:#DCDCDC;" /><br></div>
<b>Figure 5 :</b> Performance du pré-entraînement en utilisant PIRL vs Jigsaw à chaque couche
</center>

- Un classifieur linéaire est utilisé à chaque couche pour sonder et calculer la précision. L'un est entraîné en utilisant PIRL et l'autre en utilisant Jigsaw (tâche consistant à résoudre un puzzle).
- Jigsaw essaie de prédire la permutation, tandis que PIRL essaie d'y être invariant. 
- Les performances de PIRL ne cessent d'augmenter, ce qui indique que la fonction est de plus en plus alignée sur les tâches de classification en aval. 
- Jigsaw essaie de retenir les informations de la tâche de prétexte, les performances plafonnent puis chutent brusquement. Il finit par ne pas être performant pour les tâches de transfert. 


<!--
## Sampling Positives and Negatives
There are many ways to obtain samples that make up related (positive) and unrelated (negative) pairs:
1. **Crops of an image:** Contrastive Predictive Coding (CPC) based models use patches from one image that are close-by as positives (related) and patches that are far-away from each other as negatives (unrelated).

2. **Crops across images:** Patches from within one image are considered related and patches from 2 different images are considered unrelated. Used in MoCo and SimCLR

3. **Videos:** Frames that are closeby in temporal space are related, and those far-away are unrelated
   
4. **Multimodal - Video and Audio:** Video sequences along with their corresponding audio are related. Video sequences from one video and audio from another are unrelated.
   
5. **Video Object Tracking:** An object is tracked through multiple frames in a video. Detection patches from the same video are related. Detection patches from different videos are unrelated.


**What is the fundamental property of Contrastive Learning that prevents trivial solutions?**

<center>
<div style="text-align: center;"><img src="{{site.baseurl}}/images/week10/10-1/CL_objective.png" style="width: 400px; background-color:#DCDCDC;" /><br></div>
Figure 6: Loss function of Contrastive Learning
</center>

The objective function of Contrastive Learning is designed to prevent trivial solutions. The aim of the function is to make sure the distance between positive embedding pairs are smaller than the distance between negative embedding pairs. Hence, a trivial solution, where a constant distance is assigned to all embedding pairs, is impossible to attain through a minimization process on the given objective function. However, research has shown that good negative pairs are crucial for Contrastive Learning to work well.
-->

## Échantillonnage des positifs et des négatifs

Il existe plusieurs façons d'obtenir des échantillons qui constituent des paires apparentées (positives) et non apparentées (négatives) :

1. **Patchs d'une image** : les modèles basés sur le codage prédictif contrastif (CPC, *Contrastive Predictive Coding* en anglais) utilisent les zones d'une image qui sont proches les unes des autres comme positives (liées) et les zones qui sont éloignées les unes des autres comme négatives (non liées).

2. **Patchs d'images** : les patchs d'une même image sont considérés comme apparentés et les patchs de 2 images différentes sont considérés comme non apparentés. Utilisé dans MoCo et SimCLR

3. **Vidéos** : les images qui sont proches dans l'espace temporel sont liées et celles qui sont éloignées ne sont pas liées.
   
4. **Multimodal - Vidéo et audio** : les séquences vidéo et les séquences audio correspondantes sont liées. Les séquences vidéo d'une vidéo et les séquences audio d'une autre vidéo ne sont pas liées.
   
5. **Pistage (*tracking*) d'objets vidéo** : un objet est suivi à travers plusieurs images d'une vidéo. Les zones de détection d'une même vidéo sont liées. Les patchs de détection de différentes vidéos ne sont pas liés.
<br>

**Quelle est la propriété fondamentale de l'apprentissage contrastif qui empêche les solutions triviales ?**

<center>
<div style="text-align: center;"><img src="{{site.baseurl}}/images/week10/10-1/CL_objective.png" style="width: 400px; background-color:#DCDCDC;" /><br></div>
<b>Figure 6 :</b> Fonction de perte de l'apprentissage contrastif
</center>

La fonction objectif de l'apprentissage contrastif est conçue pour éviter les solutions triviales. 
L'objectif de la fonction est de s'assurer que la distance entre les paires d'enchâssements positives est inférieure à la distance entre les paires d'enchâssements négatives. 
Par conséquent, une solution triviale où une distance constante est attribuée à toutes les paires d'enchâssements est impossible à atteindre par un processus de minimisation de la fonction objective donnée. 
Cependant, les recherches ont montré que de bonnes paires négatives sont cruciales pour que l'apprentissage contrastif fonctionne bien.


<!--
## SimCLR
In SimCLR, Simple Framework for Contrastive Learning of Visual Representations, two correlated views of an image are created using augmentations like random cropping, resize, color distortions and Gaussian blur. A base encoder is used along with a projection head to obtain representations that are used to perform contrastive learning. 

It utilizes a large batch size to generate negative samples.  The batch can be spread across multiple GPUs independently. Images from one GPU could be considered as negatives for those from another GPU.

**Advantages of SimCLR:**
1. Simple to implement

**Disadvantages of SimCLR:**
1. Large batch size required
2. Large number of GPUs required

**How do we solve the compute problem in the SimcLR?**

A **Memory bank** can be used to maintain a momentum of activations across all features. With each forward pass of an image, the memory bank features are updated using the new embeddings. These memory bank features can then be used as negatives for contrastive loss. 

#### Advantages of Memory bank:
- Compute efficient. Requires one Forward Pass to compute the embeddings.

#### Drawbacks of Memory bank:
- Not online. Features become stale very fast.
- Requires large amount of GPU RAM.
- Hard to scale storage to millions of images.
-->

## SimCLR
Dans SimCLR, deux vues corrélées d'une image sont créées en utilisant des augmentations telles que des patchs aléatoire, le redimensionnement, les distorsions de couleur et le flou gaussien. 
Un encodeur de base est utilisé avec une tête de projection pour obtenir des représentations qui sont utilisées pour effectuer un apprentissage contrastif. 

Il utilise une grande taille de batch pour générer des échantillons négatifs. Le batch peut être réparti sur plusieurs GPUs de manière indépendante. 
Les images d'un GPU peuvent être considérées comme négatives pour celles d'un autre GPU.

**Avantages de SimCLR :**
1. Simple à implémenter

**Désavantages de SimCLR :**
1. Une grande taille de batch est requise
2. Un grand nombre de GPU est requis

**Comment résoudre le problème de calcul dans SimcLR ?**

Une **banque mémoire** peut être utilisée pour maintenir un *momentum* d'activations sur toutes les caractéristiques. 
À chaque passage en avant d'une image, les caractéristiques de la banque mémoire sont mises à jour en utilisant les nouveaux enchâssements. 
Les caractéristiques de la banque mémoire peuvent ensuite être utilisées comme négatives pour la perte contrastive. 

#### Avantages de la banque mémoire :
- Efficace en termes de calcul. Nécessite une seule passe en avant pour calculer les enchâssements.

#### Inconvénients de la banque mémoire :
- Pas en ligne. Les fonctionnalités deviennent très vite obsolètes.
- Nécessite une grande quantité de mémoire vive du GPU.
- Difficile de faire évoluer le stockage vers des millions d'images.

<!--
## MoCo

MoCo, or, Momentum Contrast, is a contrastive method that utilizes a memory bank to maintain a momentum of activations. It works by using 2 encoders. The $f_{\vtheta}$ encoder is the encoder that needs to be learnt. $f_{\vtheta_{\text{EMA}}}$ encoder maintains an exponential moving average of $f_{\vtheta}$.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/moco.png" style="width: 600px; background-color:#DCDCDC;" /><br>
Figure 7: Architecture of MoCo
</center>

During Forward Pass, each sample is forwarded through both encoders. Some of the embeddings from $f_{\vtheta_{\text{EMA}}}$ are kept aside for use as negative embeddings. For the algorithm, original embeddings comes from $f_{\vtheta}$, positive embeddings from $f_{\vtheta_{\text{EMA}}}$ and negative embeddings come from the set of stored embeddings.

#### Advantages of MoCo:
- Can easily be scaled in terms of memory usage as the full dataset need not be stored.
- Is an online solution, as the moving average is continuously updated.
 
#### Drawbacks of MoCo:
- Two Forward Passes are required, one through $f_{\vtheta}$ and another through $f_{\vtheta_{\text{EMA}}}$.
- Extra memory required for parameters/stored features.
-->

## MoCo

MoCo est une méthode contrastive qui utilise une banque mémoire pour maintenir un *momentum* d'activations.
Elle fonctionne en utilisant 2 encodeurs. L'encodeur $f_{\vtheta}$  est l'encodeur qui doit être appris. 
L'encodeur $f_{\vtheta_{\text{EMA}}}$ maintient une moyenne mobile exponentielle de $f_{\vtheta}$.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/moco.png" style="width: 600px; background-color:#DCDCDC;" /><br>
<b>Figure 7 :</b> Architecture de MoCo
</center>

Pendant la passe avant, chaque échantillon est passé à travers les deux encodeurs.
Certains des enchâssements de $f_{\vtheta_{\text{EMA}}}$ sont gardés de côté pour être utilisés comme enchâssements négatifs. 
Pour l'algorithme, les enchâssements originales proviennent de $f_{\vtheta}$, les enchâssements positives de $f_{\vtheta_{\text{EMA}}}$ et les enchâssements négatives proviennent de l'ensemble des enchâssements stockées.

#### Avantages de MoCo :
- Peut être facilement adapté en termes d'utilisation de la mémoire car il n'est pas nécessaire de stocker l'ensemble des données.
- C'est une solution en ligne, car la moyenne mobile est continuellement mise à jour.
 
#### Inconvénients de MoCo :
- Deux passes avant nécessaires, une par $f_{\vtheta}$ et une autre par $f_{\vtheta_{\text{EMA}}}$.
- Mémoire supplémentaire requise pour les paramètres/caractéristiques stockés.


<!--
## Clustering methods
**How CL and clustering are related to one another?**

In Constrastive Learning, each sample has corresponding positives and negatives, and the aim of the learning task is to try and bring together positive embeddings, while pushing apart negative embeddings. Essentially we are creating groups within the feature space.

Clustering, on the other hand, is a more direct way to achieve grouping, as it naturally creates groups in the feature space.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/contrastive-learning.png" style="width: 350px; background-color:#DCDCDC;" />
<img src="{{site.baseurl}}/images/week10/10-1/clustering.png" style="width: 250px; background-color:#DCDCDC;" /><br>
Figure 8: Difference between Contrastive learning (Left) and Clustering (Right)
</center>
-->

## Méthodes de *clustering*
**Comment l'apprentissage contrastif et le *clustering* sont liés l'un à l'autre ?**

Dans l'apprentissage constrastif, chaque échantillon comporte des éléments positifs et négatifs correspondants.
Le but de la tâche d'apprentissage est d'essayer de rassembler les enchâssemnts positifs tout en repoussant les enchâssemnts négatifs.
Nous créons donc essentiellement des groupes dans l'espace des caractéristiques.

Le *clustering* est un moyen plus direct de réaliser le regroupement car il crée naturellement des groupes dans l'espace des caractéristiques.

<center>
<img src="{{site.baseurl}}/images/week10/10-1/contrastive-learning.png" style="width: 350px; background-color:#DCDCDC;" />
<img src="{{site.baseurl}}/images/week10/10-1/clustering.png" style="width: 250px; background-color:#DCDCDC;" /><br>
 <b>Figure 8 :</b> Différence entre l'apprentissage contrastif (à gauche) et le <i>clustering</i> (à droite)
</center>

<!--
## SwAV
The SwAV (Swapping Assignments between Views) algorithm is an online clustering method. The idea here is to maximize the similarity of a given image $I$ and augmentation of that image $\text{augment}(I)$ and, in doing so, ensure they belong to the same cluster.

<div class="MathJax_Display" style="text-align: center;">
$$
\begin{aligned}
    f_{\vtheta}(I) = f_{\vtheta}(\text{augment}(I))
\end{aligned}
$$
</div>

Samples and their augmentations are represented by different shades of the same color. The similarity of each sample's embedding with each of the prototypes (cluster-centers) is computed. The sample is then assigned to the prototype with the highest similarity. The aim of the network is to assign an image and its augmentation to the same prototype.

**Can it lead to any trivial solution?**

A trivial solution is possible where every sample is assigned to the same group/cluster/prototype. This would cause a collapsed representation for any given input.

**How to avoid trivial solution?**
1. **Equipartition constraint**
   
    <center>
    <img src="{{site.baseurl}}/images/week10/10-1/equipartition.png" style="width: 350px; background-color:#DCDCDC;" /><br>
    Figure 9: Clustering using an equipartition constraint
    </center>

    Given n samples and k partitions, each cluster is allowed to have a maximum of n/k samples. Embeddings are equally partitioned and this prevents the single cluster trivial solution. Optimal-transport based clustering methods such as the Sinkhorn-Knopp algorithm inherently guarantees the constraint.


2. **Soft assignment**
   
    <center>
    <img src="{{site.baseurl}}/images/week10/10-1/soft-assignment.png" style="width: 450px; background-color:#DCDCDC;" /><br>
    Figure 10: Clustering using soft assignment
    </center>
    In soft assignment, a sample belongs to all prototypes with some proportion of each, such that they all sum up to 1.0. The composition values can be treated as a code that indicates how each embedding is encoded in the 'prototype' space.

**Consider the case where we have a high class imbalance. If we use the equipartition method, then won't that give inacurate results?**

Soft assignment solves this problem by representing a lot more classes (logically) than hard assignment. It gives a richer representation and is less sensitive to $k$ (number of classes) and hence $k/n$. However, if hard assignment is used, the class imbalance and the fixed value of $n/k$ will create inaccuracies.

#### Training SwAV:

<center>
<img src="{{site.baseurl}}/images/week10/10-1/swav.png" style="width: 600px; background-color:#DCDCDC;" /><br>
Figure 11: Architecture of SwAV
</center>

2 crops from an image are passed through the network to compute the corresponding codes. The $2^{\text{nd}}$ code is then predicted from the $1^{\text{st}}$ embedding and vice versa. This task forces the network to become invariant to data augmentations. Gradients are backpropagated onto the prototypes as well. Hence this model is updated in an online manner.

### Advantages of SwAV:
1. No explicit negatives needed
2. Optimal transport methods avoid trivial solutions.
3. Faster convergence than contrastive learning. 
    - The code space imposes more constraints and the embeddings are not directly compared.
4. Less compute requirements.
5. Smaller number (4-8) of GPUs required.
-->

## SwAV
L'algorithme SwAV (*Swapping Assignments between Views*) est une méthode de *clustering* en ligne. 
L'idée ici est de maximiser la similarité d'une image donnée $I$ et de l'augmentation de cette image $\text{augment}(I)$ et, ce faisant, de s'assurer qu'elles appartiennent au même cluster.

<div class="MathJax_Display" style="text-align: center;">
$$
\begin{aligned}
    f_{\vtheta}(I) = f_{\vtheta}(\text{augment}(I))
\end{aligned}
$$
</div>

Les échantillons et leurs augmentations sont représentés par différentes nuances de la même couleur. 
La similarité de l'enchâssement de chaque échantillon avec chacun des prototypes (centres des clusters) est calculée.
L'échantillon est alors assigné au prototype ayant la plus grande similarité. L'objectif du réseau est d'affecter une image et son augmentation au même prototype.

**Peut-il conduire à une solution triviale ?**

Une solution triviale est possible lorsque chaque échantillon est affecté au même groupe/cluster/prototype. Cela entraîne une représentation effondrée pour toute entrée donnée.

**Comment éviter la solution triviale ?**
1. **Contrainte d'équipartition**
   
    <center>
    <img src="{{site.baseurl}}/images/week10/10-1/equipartition.png" style="width: 350px; background-color:#DCDCDC;" /><br>
    <b> Figure 9 :</b> <i>Clustering</i> à l'aide d'une contrainte d'équipartition
    </center>

    Étant donné $n$ échantillons et $k$ partitions, chaque cluster est autorisé à avoir un maximum de $n/k$ échantillons. 
    Les enchâssements sont partitionnés de manière égale ce qui empêche la solution triviale du cluster unique.
    Les méthodes de *clustering* basées sur le transport optimal, comme l'algorithme de Sinkhorn-Knopp, garantissent intrinsèquement la contrainte.


2. **Affectation douce**
   
    <center>
    <img src="{{site.baseurl}}/images/week10/10-1/soft-assignment.png" style="width: 450px; background-color:#DCDCDC;" /><br>
    <b>Figure 10 :</b> <i>Clustering</i> à l'aide de l'affectation douce
    </center>
    Dans l'affectation douce, un échantillon appartient à tous les prototypes avec une certaine proportion de chacun d'entre eux, de sorte que leur somme soit égale à $1$.
    Les valeurs de composition peuvent être traitées comme un code qui indique comment chaque enchâssement est codé dans l'espace prototype.


**Considérons le cas où nous avons un déséquilibre de classe élevé. Si nous utilisons la méthode d'équipartition, cela ne donnera-t-il pas des résultats inexacts ?**

L'affectation douce résout ce problème en représentant beaucoup plus de classes que l'affectation dure. 
Elle donne une représentation plus riche et est moins sensible à $k$ (nombre de classes) et donc à $k/n$. 
Cependant, si l'affectation dure est utilisée, le déséquilibre des classes et la valeur fixe de $n/k$ créeront des imprécisions.

#### Entraînement de SwAV :

<center>
<img src="{{site.baseurl}}/images/week10/10-1/swav.png" style="width: 600px; background-color:#DCDCDC;" /><br>
<b>Figure 11 :</b> Architecture de SwAV
</center>

Deux patchs d'une image sont passés par le réseau pour calculer les codes correspondants.
Le code $2$ est alors prédit à partir de l'enchâsement $1$ et vice versa. Cette tâche force le réseau à devenir invariant aux augmentations de données.
Les gradients sont également rétropropagés sur les prototypes. Ce modèle est donc mis à jour en ligne.

### Avantages de SwAV :
1. Pas besoin de négations explicites
2. Les méthodes de transport optimales évitent les solutions triviales
3. Convergence plus rapide que l'apprentissage contrastif  
    - L'espace des codes impose plus de contraintes et les enchâsements ne sont pas directement comparés
4. Moins d'exigences en matière de calcul
5. Un plus petit nombre (4-8) de GPUs est nécessaire


<!--
## Pretraining on ImageNet without labels:
Even though ImageNet without labels can be used for self supervised learning, there is an inherent bias in the dataset due to the curation/hand-selection process involved.
1. Images belong to 1000 specific classes
2. Images contain a prominent object
3. Images have very limited clutter and very few background concepts
-->

## Pré-entraînement sur ImageNet sans étiquettes :
Même si ImageNet sans étiquettes peut être utilisé pour l'apprentissage autosupervisé, il existe un biais inhérent dans le jeu de données en raison du processus de sélection impliqué.
1. Les images appartiennent à $1000$ classes spécifiques
2. Les images contiennent un objet proéminent
3. Les images ont un encombrement très limité et très peu de concepts de fond

<!--
## Pretraining on Non-ImageNet data

Pretraining on Non-ImageNet data hurts performance.
<center>
<img src="{{site.baseurl}}/images/week10/10-1/non-imagenet.png" style="width: 600px; background-color:#DCDCDC;" /><br>
Figure 12: Example of a problematic image from a non-imagenet dataset
</center>
Consider the above image. One crop has a refrigerator and the other crop could be that of a table/chair. The algorithm would try to create similar embeddings for both of them, because they belong to the same image. This is not what we expect from the training process.

Real world data has very different distributions. They may sometimes even be cartoon images or memes, and there is no guarantee that an image will contain a single (or sometimes, any) prominent object.
-->

## Pré-entraînement sur des données non-ImageNet

Le pré-entraînement sur des données non-ImageNet nuit aux performances.
<center>
<img src="{{site.baseurl}}/images/week10/10-1/non-imagenet.png" style="width: 600px; background-color:#DCDCDC;" /><br>
<b>Figure 12 :</b> Exemple d'une image problématique provenant d'un jeu de données n'étant pas Imagenet
</center>

Considérons l'image ci-dessus. Un patch présente un réfrigérateur et l'autre pourrait être celui d'une table/chaise. 
L'algorithme essaie de créer des enchâssements similaires pour les deux car ils appartiennent à la même image. Ce n'est pas ce que nous attendons du processus d'entraînement.

Les données du monde réel ont des distributions très différentes. 
Il peut même s'agir d'images de dessins animés ou de memes et rien ne garantit qu'une image contiendra un seul (ou parfois, n'importe quel) objet proéminent.
