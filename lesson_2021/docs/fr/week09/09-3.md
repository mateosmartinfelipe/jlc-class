---
lang: fr
lang-ref: ch.9-3
title: Modèles génératifs
lecturer: Alfredo Canziani
authors: Vasudev Awatramani, Sumit Mamtani
date: 19 May 2021
typora-root-url: 09-3
translation-date: 19 Jun 2021
translator: Loïck Bourdois
---     


<!--
## [Autoencoders (AE)](https://www.youtube.com/watch?v=bZF4N8HR1cc&t=204s)

Autoencoders are artificial neural networks trained in an unsupervised fashion that aim to learn representation of input data and then generate the data from the learned encoded representations. Autoencoders can be considered a special case of Amortized Inference where instead of finding the optimal latent to produce appropriate reconstruction of the input, we simply feed the output of encoder to decoder.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_1_ae.png" height="400px" /><br>
<b>Fig. 1</b>: General Architeture of an Autoencoder
</center>

We can express the above architecture mathematically as

$$
\vh=f(\mW{_h}\vy+\vb{_h})\\
\vytilde=g(\mW{_y}\vh+\vb{_y})\\
$$

With following dimensionsionalitites:

$$

\vy,\vytilde \in \mathbb{R}^n\\
\vh \in \mathbb{R}^d \\
\mW{_h} \in \mathbb{R}^{d \times n}\\
\mW{_y} \in \mathbb{R}^{n \times d} \\
$$

The data is represented only through $\vy$ as the aim is to reconstruct the data that lives on the manifold (instance of unconditional EBM) through following energy function:

$$
\red{F}(\vy)=\red{C}({\vy},\vytilde)+\textcolor{#ff666d}{R}(\vh)
$$
-->


## [Auto-encodeurs (AE)](https://www.youtube.com/watch?v=bZF4N8HR1cc&t=204s)

Les auto-encodeurs sont des réseaux neuronaux artificiels entraînés de manière non supervisée qui visent à apprendre la représentation des données d'entrée, puis à générer les données à partir des représentations codées apprises.
Les auto-encodeurs peuvent être considérés comme un cas particulier d'inférence amortie où, au lieu de trouver la latente optimale pour produire une reconstruction appropriée de l'entrée, nous alimentons simplement la sortie de l'encodeur au décodeur.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_1_ae.png" height="400px" /><br>
<b>Figure 1 :</b> Architeture générale d'un auto-encodeur
</center>

Nous pouvons exprimer mathématiquement l'architecture ci-dessus comme suit :

$$
\vh=f(\mW{_h}\vy+\vb{_h})\\
\vytilde=g(\mW{_y}\vh+\vb{_y})\\
$$

Avec les problèmes de dimensions suivants :

$$
\vy,\vytilde \in \mathbb{R}^n\\
\vh \in \mathbb{R}^d \\
\mW{_h} \in \mathbb{R}^{d \times n}\\
\mW{_y} \in \mathbb{R}^{n \times d} \\
$$

Les données sont représentées uniquement par $\vy$ car le but est de reconstruire les données qui vivent sur la variété (instance d'EBM inconditionnelle) par la fonction d'énergie suivante :

$$
\red{F}(\vy)=\red{C}({\vy},\vytilde)+\textcolor{#ff666d}{R}(\vh)
$$

<!--
### Under/Over Complete Autoencoder

In an autoencoder, when the dimension of hidden representation $d$, ($30$) is less than that of input size, $n$ ($784$), it can be referred as *Under Complete Autoencoder*.

Correspondingly, when the dimensionality of hidden representation, $d$ is greater than input dimensions $n$, it is said to be *Over Complete Autoencoder*.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_2_under_over.png" height="400px" /><br>
<b>Fig. 2</b>: Under Complete (left) and Over-Complete Autoencoder (right)
</center>

We are working with MNIST $28 \times 28$ images such that the transfomation routine is defined from $784 (=28 \times 28) \rightarrow 30$ under the encoder followed by decoder that maps $30 \rightarrow 784$. The images are normalized and pixel values $\in [-1,1]$

```python
class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, d),
            nn.Tanh(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(d, 28 * 28), # Rotation
            nn.Tanh(), # Squashing
        )

    def forward(self, y):
        h = self.encoder(y) # 784 -> 30
        y_tilde = self.decoder(h) # 30 -> 784
        return y_tilde

model = Autoencoder()
criterion = nn.MSELoss()
```

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_3_ae_outputs.png" height="600px" /><br>
<b>Fig. 3</b>: Output Generated by Under Complete Autoencoder.
</center>

Let's have a look on some of the kernels of the encoder.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_4_autoencoder_kernel.png" height="600px" /><br>
<b>Fig. 4</b>: Kernel Outputs of Under Complete Autoencoder.
</center>

If you notice there appears *salt and pepper noise* surrounding the center of images. This is due to the low spatial frequency along the region in the input image. The $+1$s and $-1$s in the noise cancel out to $0$, resulting in their average contribution as $0$. High-frequency regions appear around the central region as the digits in MNIST images are centered. The kernels with only noise indicate that the corresponding kernel collapsed or died.
-->


### Auto-encodeur sous/sur complet

Dans un auto-encodeur, lorsque la dimension de la représentation cachée, $d$, ($30$) est inférieure à celle de la taille de l'entrée, $n$ ($784$), on peut parler d'auto-encodeur sous-complet.

Corrélativement, lorsque la dimension de la représentation cachée, $d$ est supérieure aux dimensions d'entrée, $n$, on dit qu'il s'agit d'un auto-encodeur sur-complet.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_2_under_over.png" height="400px" /><br>
<b>Figure 2 :</b> Auto-encodeur sous-complet (à gauche) et sur-complet (à droite)
</center>

Nous travaillons avec des images MNIST $28 \times 28$ de telle sorte que la routine de transfomation est définie de $784 (=28 \times 28) \rightarrow 30$ sous l'encodeur suivi du décodeur qui fait correspondre $30 \rightarrow 784$. Les images sont normalisées et les valeurs des pixels $\in [-1,1]$.


```python
class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28 * 28, d),
            nn.Tanh(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(d, 28 * 28), # Rotation
            nn.Tanh(), # Squashing
        )

    def forward(self, y):
        h = self.encoder(y) # 784 -> 30
        y_tilde = self.decoder(h) # 30 -> 784
        return y_tilde

model = Autoencoder()
criterion = nn.MSELoss()
```

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_3_ae_outputs.png" height="600px" /><br>
<b>Figure 3 :</b> Sortie générée par l'auto-encodeur sous-complet
</center>

Jetons un coup d'œil sur certains des noyaux de l'encodeur.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_4_autoencoder_kernel.png" height="600px" /><br>
<b>Figure 4 :</b> Sorties du noyau de l'auto-encodeur sous-complet
</center>

Si vous remarquez qu'il apparaît *un bruit de sel et de poivre* entourant le centre des images, cela est dû à la faible fréquence spatiale le long de la région dans l'image d'entrée.
Les $+1$ et $-1$ du bruit s'annulent à $0$, ce qui fait que leur contribution moyenne est de $0$.
Des régions à haute fréquence apparaissent autour de la région centrale, car les chiffres des images MNIST sont centrés.
Les noyaux avec seulement du bruit indiquent que le noyau correspondant s'est effondré ou est mort.



<!--
## [Denoising Autoencoder (DAE)](https://www.youtube.com/watch?v=bZF4N8HR1cc&t=941s)
In Denosing Autoencoder, we take a sample from dataset, inject some noise such the autoencoder is forced to reproduce the original sample. Therefore, the aim is to learn the vector field that should transform corrupted sample back to denoised part. Here, we set $d=500$ which is greater than number of pixels actually utilized to represent the digits in images. (Over Complete AE)

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_5_dae.png"  height="400px" /><br>
<b>Fig. 5</b>: Denoising Autoencoder Architecture. (Intution on left, transform corrupted $\vyhat$ towards the data manifold of $\vy$)
</center>

For the purpose of adding noise we perform the following steps:
1. Employ `do=nn.Dropout()` to randomly turn neuron off i.e. some pixel values will result in 0. (Original image is composed of pixel values $[-1,1]$)
2. Create a noise mask using the dropout `do(torch.ones(img.shape))`
3. Generate corrupted image by multiplying the original image with noise mask. `img_bad=(img*noise)`

The criterion for the model stay same i.e. to reproduce original sample given a noisy sample.

Again, let's have a look at the kernels of the encoder. As you can see there is no salt and pepper noise as the surrounding region is no longer zero-mean and the kernel is forced to learn to ignore the region out of interest.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_6_dae_kernels.png" height="600px" /><br>
<b>Fig. 6</b>: Kernels of Denoising Autoencoder.
</center>

Comparing our denoised autoencoder with Computer Vision Inpainting methods such as Telea and Navier-Stokes methods.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/dae_noise.png"/><br>
Noise
<img src="{{site.baseurl}}/images/week09/09-3/noise_input.png"/><br>
Noised Input
<img src="{{site.baseurl}}/images/week09/09-3/dae_output.png" /><br>
Denoised Output by DAE
<img src="{{site.baseurl}}/images/week09/09-3/telea_output.png" /><br>
Telea Inpainting Output
<img src="{{site.baseurl}}/images/week09/09-3/ns_output.png" /><br>
Naiver-Stoke Inpainting Output<br>
<b>Fig. 7</b>: Comparison of outputs of Denoised Autoencoder and state-of-the-art Computer Vision Inpainting algorithms.
</center>

Recall that an Denoised Autoencoder is an Constrastive EBM that assigns low-energy to samples lying on the actual data manifold (observed during training). Now, to test this out we merge two digits (perform alpha composite) and pass through the autoencoder:

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_8_merged_imgs.png" height="300px" /><br>
<b>Fig. 8</b>: Garbage (merged digits) on left and corresponding DAE's output. DAE as expected fails to denoise the image (this is a good thing!).
</center>

Interestingly, the autoencoder fails to reconstruct the merged garbage input as it was not observed during training. Therefore, the autoencoder can be used to estimate how noisy a given input sample is.
-->

## [Auto-encodeur débruiteur (DAE)](https://www.youtube.com/watch?v=bZF4N8HR1cc&t=941s)
Dans l'auto-encodeur débruiteur, nous prenons un échantillon d'un ensemble de données, nous injectons un certain bruit de sorte que l'auto-encodeur est obligé de reproduire l'échantillon original. 
Par conséquent, l'objectif est d'apprendre le champ vectoriel qui doit transformer l'échantillon corrompu en une partie débruitée. 
Ici, nous avons fixé $d=500$, ce qui est supérieur au nombre de pixels réellement utilisés pour représenter les chiffres dans les images (AE sur-complet).

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_5_dae.png"  height="400px" /><br>
<b>Figure 5 :</b> Architecture de l'auto-encodeur débruiteur. (Intution à gauche, transformer les $\vyhat$ corrompus vers la variété de données de $\vy$)
</center>

Dans le but d'ajouter du bruit, nous effectuons les étapes suivantes :
1. Employer `do=nn.Dropout()` pour désactiver aléatoirement le neurone, c'est-à-dire que certaines valeurs de pixel donneront 0. (L'image originale est composée de valeurs de pixel $[-1,1]$).
2. Créer un masque de bruit en utilisant le dropout `do(torch.ones(img.shape))`.
3. Générer une image corrompue en multipliant l'image originale avec le masque de bruit. `img_bad=(img*noise)`

Le critère du modèle reste le même, c'est-à-dire reproduire l'échantillon original à partir d'un échantillon bruité.

Encore une fois, regardons les noyaux de l'encodeur. Comme vous pouvez le voir, il n'y a pas de bruit *poivre et sel* car la région environnante n'est plus de moyenne zéro et le noyau est obligé d'apprendre à ignorer la région hors d'intérêt.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_6_dae_kernels.png" height="600px" /><br>
<b>Figure 6 :</b> Noyaux de l'auto-encodeur débruiteur
</center>

Comparaison de notre DAE avec les méthodes d'*Inpainting* de vision par ordinateur telles que les méthodes Telea et Navier-Stokes.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/dae_noise.png"/><br>
Bruit
<img src="{{site.baseurl}}/images/week09/09-3/noise_input.png"/><br>
Entrée bruitée
<img src="{{site.baseurl}}/images/week09/09-3/dae_output.png" /><br>
Sortie débruitée par le DAE
<img src="{{site.baseurl}}/images/week09/09-3/telea_output.png" /><br>
Sortie de Telea
<img src="{{site.baseurl}}/images/week09/09-3/ns_output.png" /><br>
Sortie de Naiver-Stoke <br>
<b>Figure 7 :</b> Comparaison des sorties du DAE et des algorithmes d'inpainting de pointe de vision par ordinateur
</center>

Rappelons qu'un DAE est un EBM constrastif qui attribue une faible énergie aux échantillons se trouvant sur la variété de données réel (observé pendant l'entraînement). 
Maintenant, pour tester cela, nous fusionnons deux chiffres (composite alpha) et passons par l'auto-encodeur :

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_8_merged_imgs.png" height="300px" /><br>
<b>Figure 8 :</b> Chiffres fusionnés à gauche et sortie correspondante du DAE. Le DAE comme prévu ne parvient pas à débruiter l'image (c'est une bonne chose !)
</center>

Il est intéressant de noter que l'auto-encodeur ne parvient pas à reconstruire l'entrée des chiffres fusionnées car elle n'a pas été observée pendant l'entraînement.
Par conséquent, l'auto-encodeur peut être utilisé pour estimer le degré de bruit d'un échantillon d'entrée donné.


<!--
## [Variational Autoencoder (VAE)](https://www.youtube.com/watch?v=bZF4N8HR1cc&t=1583s)

Varitational Autoencoders are type of generative models, where we aim to represent latent attribute for given input as a probability distribution. The encoder produces $\vmu$ and $\vv$ such that a sampler samples a latent input $\vz$ from these encoder outputs. The latent input $\vz$ is simply fed to encoder to produce $\vyhat$ as reconstruction of $\vy$.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_9_vae.png"  height="400px"  /><br>
<b>Fig. 9</b>: Variational Autoencoder Intution
</center>

Here, we consider the latent random variable as $\vz$ belonging to a Gaussian with mean $\vmu$ and variance $\vy$. (Feel free to use any other distribution). Unlike before, we do not normalize the images.
-->

## [Auto-encodeur variationnel (VAE)](https://www.youtube.com/watch?v=bZF4N8HR1cc&t=1583s)

Les auto-encodeurs variationnels sont un type de modèles génératifs où l'on cherche à représenter un attribut latent pour une entrée donnée sous la forme d'une distribution de probabilité. 
L'encodeur produit $\vmu$ et $\vv$ de sorte qu'un échantillonneur échantillonne une entrée latente $\vz$ à partir de ces sorties d'encodeur.
L'entrée latente $\vz$ est simplement introduite dans l'encodeur pour produire $\vyhat$ comme reconstruction de $\vy$.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_9_vae.png"  height="400px"  /><br>
<b>Figure 9 :</b> Intution de l'auto-encodeur variationnel
</center>

Ici, nous considérons la variable latente aléatoire $\vz$ appartenant à une gaussienne de moyenne $\vmu$ et de variance $\vy$. (N'hésitez pas à utiliser toute autre distribution).
Contrairement à la version précédente, nous ne normalisons pas les images.


<!--
### Encoder and Decoder

The last layer of encoder has an output of dimension $2d$: first $d$ values refer to means, $\vmu$ and remaninig $d$ values are variances $\vv$. The decoder has a sigmoid activation for last layer to maintain output range $[0,1]$.

```python
d = 20

class VAE(nn.Module):
    def __init__(self):
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Linear(784, d ** 2),
            nn.ReLU(),
            nn.Linear(d ** 2, d * 2)
        )

        self.decoder = nn.Sequential(
            nn.Linear(d, d ** 2),
            nn.ReLU(),
            nn.Linear(d ** 2, 784),
            nn.Sigmoid(),
        )
```
-->


### Encodeur et décodeur

La dernière couche de l'encodeur a une sortie de dimension $2d$ : les premières valeurs $d$ font référence aux moyennes, $\vmu$ et les dernières valeurs $d$ sont des variances $\vv$.
Le décodeur a une activation sigmoïde pour la dernière couche afin de maintenir l'intervalle de sortie $[0,1]$.

```python
d = 20

class VAE(nn.Module):
    def __init__(self):
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Linear(784, d ** 2),
            nn.ReLU(),
            nn.Linear(d ** 2, d * 2)
        )

        self.decoder = nn.Sequential(
            nn.Linear(d, d ** 2),
            nn.ReLU(),
            nn.Linear(d ** 2, 784),
            nn.Sigmoid(),
        )
```


<!--
### Reparamaterise and forward function

During trainig, the `reparameterise` function is used for the *reparameterization trick*: We cannot backpropagate through the sampler, we simply compute $\vz=\vmu+\epsilon\odot\sqrt{\vv}$ where $\epsilon \in \orange{\mathcal{N}}(0,\mathbb{I}_d)$. This allows to flow gradient back to encoder. During test time, we simply use $\vmu$

```python
def reparameterise(self, mu, logvar):
    if self.training:
        std = logvar.mul(0.5).exp_()
        epsilon = std.data.new(std.size()).normal_()
        return eps.mul(std).add_(mu)
    else:
        return mu

def forward(self, x):
    mu_logvar = self.encoder(x.view(-1, 784)).view(-1, 2, d)
    mu = mu_logvar[:, 0, :]
    logvar = mu_logvar[:, 1, :]
    z = self.reparameterise(mu, logvar)
    return self.decoder(z), mu, logvar
```

We use log variance instead of varaince (change of scale) because we want to ensure
1. Variance is non-negative.
2. Full range of variance, to make training stable.

Recall the free energy for VAE,

$$
    \red{\tilde{F}}(\vy)=\red{C}(\vy,\vytilde)
    +\beta \red{D}_{KL}[\textcolor{#f2ac5d}{\orange{\mathcal{N}}}(\vmu,\vv)\mathrel{\Vert}\orange{\mathcal{N}}(0,1)]\\
    =\red{C}(\vy,\vytilde)+\frac{\beta}{2}\sum_{i=1}^{d}\green{v_i}-\log(\green{v_i})-1+\green{\mu_i}^2
$$

To regualrize the expressivity of the latent, we include KL-divergence between the Gaussian of latent variable and a Normal distribution ($\orange{\mathcal{N}}(0,1))$.
(Also see [Week 8-Practicum]({{site.baseurl}}/images/week8/08-3) for *bubble* explaination of VAE loss)

Therefore, we define the loss function as

```python
def loss_function(x_hat, x, mu, logvar, β=1):
    BCE = nn.functional.binary_cross_entropy(
        x_hat, x.view(-1, 784), reduction='sum'
    )
    KLD = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))

    return BCE + β * KLD
```

Since, VAE is a generative model, we sample from the distribution to generate the following digits:

```python
N = 16
z = torch.rand((N,d))
sample = model.decoder(z)
```

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_10_cluster_samples.png" style="background-color:#DCDCDC;" /><br>
<b>Fig. 10</b>: TSNE visualization of samples generated through VAE.
</center>

The regions (classes) get segregated as the reconstruction term forces the latent space to get well defined. The data gets clustered into classes without actually using the labels.
-->

### Reparamétrage et fonction forward

Pendant l'apprentissage, la fonction `reparameterise` est utilisée pour l'astuce de *reparamétrage*.
Nous ne pouvons pas rétropropager à travers l'échantillonneur, nous calculons simplement $\vz=\vmu+\epsilon\odot\sqrt{\vv}$ où $\epsilon \in \orange{\mathcal{N}}(0,\mathbb{I}_d)$.
Cela permet de renvoyer le gradient vers l'encodeur. Pendant le test, on utilise simplement $\vmu$.

```python
def reparameterise(self, mu, logvar):
    if self.training:
        std = logvar.mul(0.5).exp_()
        epsilon = std.data.new(std.size()).normal_()
        return eps.mul(std).add_(mu)
    else:
        return mu

def forward(self, x):
    mu_logvar = self.encoder(x.view(-1, 784)).view(-1, 2, d)
    mu = mu_logvar[:, 0, :]
    logvar = mu_logvar[:, 1, :]
    z = self.reparameterise(mu, logvar)
    return self.decoder(z), mu, logvar
```

Nous utilisons la variance logarithmique au lieu de la variance (changement d'échelle) car nous voulons nous assurer que :
1. la variance est non-négative,
2. avoir une gamme complète pour la variance afin de rendre l'entraînement stable.

Rappelons l'énergie libre pour le VAE : 


$$
    \red{\tilde{F}}(\vy)=\red{C}(\vy,\vytilde)
    +\beta \red{D}_{KL}[\textcolor{#f2ac5d}{\orange{\mathcal{N}}}(\vmu,\vv)\mathrel{\Vert}\orange{\mathcal{N}}(0,1)]\\
    =\red{C}(\vy,\vytilde)+\frac{\beta}{2}\sum_{i=1}^{d}\green{v_i}-\log(\green{v_i})-1+\green{\mu_i}^2
$$

Pour régulariser la variable latente, nous incluons la divergence KL entre la gaussienne de la variable latente et une distribution normale ($\orange{\mathcal{N}}(0,1))$.
(Voir également [la semaine 8]({{site.baseurl}}/images/week8/08-3) pour une explication sous forme de *bulles* de la perte VAE).

Par conséquent, nous définissons la fonction de perte comme suit :

```python
def loss_function(x_hat, x, mu, logvar, β=1):
    BCE = nn.functional.binary_cross_entropy(
        x_hat, x.view(-1, 784), reduction='sum'
    )
    KLD = 0.5 * torch.sum(logvar.exp() - logvar - 1 + mu.pow(2))

    return BCE + β * KLD
```

Puisque le VAE est un modèle génératif, nous échantillonnons à partir de la distribution pour générer les chiffres suivants :

```python
N = 16
z = torch.rand((N,d))
sample = model.decoder(z)
```

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_10_cluster_samples.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 10 :</b> Visualisation T-SNE des échantillons générés par le VAE
</center>

Les classes sont séparées car le terme de reconstruction force l'espace latent à être bien défini. Les données sont regroupées en classes sans utiliser réellement les étiquettes.


<!--
## [Generative Adversarial Nets (GANs)](https://www.youtube.com/watch?v=bZF4N8HR1cc&t=2578s)

GANs have the same feel as DAE with some tweaks. DAE involves generating corrput samples using the input and a distribution followed by denoising them using a decoder. GANs instead directly sample the distribution (without the input) to produce output $\vyhat$ using the Generator (or decoder in DAE terms). The input $\vy$ and $\vyhat$ are provided to the Cost network separately to measure incomptability between them.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_11_gan_vs_dae.png" height="400px" /><br>
<b>Fig. 11</b>: Comparision of GANs (left) and DAE (right) architectures.
</center>

Similarly, we can extend the analogy with VAE. In contrast to VAE where the sampler is conditioned on the output of the encoder, in GANs there is an unconditioned sampler and again, decoder corresponds to Generator.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_12_gan_vs_vae.png" height="400px" /><br>
<b>Fig. 12</b>: Comparision of VAE(left) and GANs (right) architectures.
</center>

The Generator maps the latent space to data space:

$$
\textcolor{#62a2af}{G}:\textcolor{#f2ac5d}{\mathcal{Z}} \rightarrow \mathbb{R}^n,\vz \rightarrow \vyhat
$$

The observed $\vy$ and $\vyhat$ are fed to a Cost network to measure incompatibility:

$$
\red{C}:\mathbb{R}^n\rightarrow \mathbb{R}, \vy \vee \vyhat \rightarrow \textcolor{#ff666d}{c}
$$
-->

## [Réseaux antagonistes génératifs (GANs pour *Generative Adversarial Networks*)](https://www.youtube.com/watch?v=bZF4N8HR1cc&t=2578s)

Les GANs ont le même aspect que les DAEs avec quelques modifications.
Le DAE implique la génération d'échantillons corrompus à partir de l'entrée et d'une distribution, suivie d'un débruitage à l'aide d'un décodeur. 
Au lieu de cela, les GANs échantillonnent directement la distribution (sans l'entrée) pour produire une sortie $\vyhat$ en utilisant le générateur (ou décodeur en termes de DAE).
Les entrées $\vy$ et $\vyhat$ sont fournies au réseau *Cost* séparément pour mesurer l'incomptabilité entre elles.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_11_gan_vs_dae.png" height="400px" /><br>
<b>Figure 11 :</b> Comparaison des architectures GANs (à gauche) et DAEs (à droite)
</center>

De la même manière, nous pouvons étendre l'analogie avec le VAE. 
Contrairement au VAE où l'échantillonneur est conditionné à la sortie de l'encodeur, dans les GANs, il y a un échantillonneur non conditionné. 
Là encore, le décodeur correspond au générateur.

<center>
<img src="{{site.baseurl}}/images/week09/09-3/fig_12_gan_vs_vae.png" height="400px" /><br>
<b>Figure 12</b> : Comparaison des architectures VAEs (à gauche) et GANs (à droite)
</center>

Le générateur fait correspondre l'espace latent à l'espace des données :

$$
\textcolor{#62a2af}{G}:\textcolor{#f2ac5d}{\mathcal{Z}} \rightarrow \mathbb{R}^n,\vz \rightarrow \vyhat
$$

Les $\vy$ et $\vyhat$ observés sont introduits dans un réseau de coût pour mesurer l'incompatibilité :

$$
\red{C}:\mathbb{R}^n\rightarrow \mathbb{R}, \vy \vee \vyhat \rightarrow \textcolor{#ff666d}{c}
$$



<!--
### Training GANs

We define the loss functional for the Cost Network (Discriminator):

$$
\ell_{\red{C}}(\vy,\vyhat)=\red{C}(\vy)+[m-\red{C}(\vyhat)]^+
$$

The aim is to push down sample of $\vy$ and push up the energy of $\vyhat$ upto $m$ (if $\red{C}\geq m$ no gradient is recieved as $\texttt{ReLU}(\cdot)$ would result the output to $0$).

$$
\ell_{\red{C}}(\vy,\vyhat)=\red{C}(\vy)+[m-\textcolor{#62a2af}{G}(\vz)]^+
$$

For training the generator, the aim is to simply minimize the cost:

$$
\ell_{\textcolor{#62a2af}{G}}(\vz)=\red{C}(\textcolor{#62a2af}{G}(\vz))
$$

A possible choice of $\red{C}(\vy)$ can be

$$
\red{C}(\vy)=\Vert{\yellow{\text{Dec}}}(\green{\text{Enc}}(\vy))-{\vy}\Vert^2
$$

The cost network would push good samples to 0 and bad sample to energy level $m$. Using the above $\red{C}(\vy)$, there would exist a quadratic distance between the points on manifold, $\vy}$ and points generated by the generator $\vyhat$. During the training, generator is updated to try to produce samples that would gradually have low energy as $\vy$ guided by $\red{C}$. Once trained, the generator should produce samples near to data manifold.

**Adopting another analogy** , the generative model can be thought as team of *counterfeiters*, trying to produce fake currency. Their aim to produce fake currency which is indistinguish from real currency. The disciminator can be viewed as *police*, trying to detect among counterfeit and fake currency bills. Gradients from backprop can be seen as *spies* that give opposite direction to counterfeiters (generator) in order to fool the police (discriminator).
-->

### Entraînement des GANs

Nous définissons la fonctionnelle de perte pour le réseau de coût (discriminateur) :

$$
\ell_{\red{C}}(\vy,\vyhat)=\red{C}(\vy)+[m-\red{C}(\vyhat)]^+
$$

Le but est de diminuer l'échantillon de $\vy$ et d'augmenter l'énergie de $\vyhat$ jusqu'à $m$ (si $\red{C}\geq m$ aucun gradient n'est reçu car $\texttt{ReLU}(\cdot)$ entraînerait la sortie à $0$).

$$
\ell_{\red{C}}(\vy,\vyhat)=\red{C}(\vy)+[m-\textcolor{#62a2af}{G}(\vz)]^+
$$

Pour l'entraînement du générateur, l'objectif est simplement de minimiser le coût :

$$
\ell_{\textcolor{#62a2af}{G}}(\vz)=\red{C}(\textcolor{#62a2af}{G}(\vz))
$$

Un choix possible de $\red{C}(\vy)$ peut être :

$$
\red{C}(\vy)=\Vert{\yellow{\text{Dec}}}(\green{\text{Enc}}(\vy))-{\vy}\Vert^2
$$

Le réseau de coût pousse les bons échantillons à 0 et les mauvais échantillons au niveau d'énergie $m$. 
En utilisant la formule ci-dessus $\red{C}(\vy)$, il existerait une distance quadratique entre les points de la variété $\vy}$ et les points générés par le générateur $\vyhat$.
Au cours de l'apprentissage, le générateur est mis à jour pour essayer de produire des échantillons qui auraient progressivement une énergie faible à mesure que $\vy$ 
est guidé par $\red{C}$. Une fois entraîné, le générateur devrait produire des échantillons proches de la variété de données.

**En adoptant une autre analogie**, le modèle génératif peut être considéré comme une équipe de *faussaires* essayant de produire de la fausse monnaie.
Leur objectif est de produire de la fausse monnaie qui ne se distingue pas de la vraie. 
Le disciminateur peut être considéré comme la *police* qui essaie de détecter les billets de banque contrefaits et faux. 
Les gradients de la rétropropagation peuvent être considérés comme des *espions* qui donnent une direction opposée aux contrefacteurs (générateur) afin de tromper la police (discriminateur).




<!--
### Implementating Deep Convolutional Generative Adversarial Nets (DCGANs)

Follow this [link](https://github.com/pytorch/examples/tree/master/dcgan) for complete code.

The Generator upsamples the input using several ```nn.ConvTranspose2d``` modules to produce image from random vector `nz` (noise).

```python
class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),
            # state size. (ngf*8) x 4 x 4
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # state size. (ngf*4) x 8 x 8
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # state size. (ngf*2) x 16 x 16
            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            # state size. (ngf) x 32 x 32
            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (nc) x 64 x 64
        )

    def forward(self, input):
        if input.is_cuda and self.ngpu > 1:
            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
        else:
            output = self.main(input)
        return output

```

Discriminator is essentially a image classifier that uses ```nn.Sigmoid()``` to classify the input as real/fake.
```python
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # input is (nc) x 64 x 64
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf) x 32 x 32
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*2) x 16 x 16
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*4) x 8 x 8
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*8) x 4 x 4
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        output = self.main(input)
        return output.view(-1, 1).squeeze(1)
```

We use Binary Cross Entropy to train the networks.
```python
criterion = nn.BCELoss()
```

We have two optimizers for each network. (We want to push up the energy of bad (recognizable fake images) samples and push down energy of good samples (real looking images).

```python
optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
```

For training, we first train the discriminator with real images and labels stating the images being real. Followed by generato generating fake images from noise. The discriminator is again trained but this time with fake images and labels stating them as fake.

```python
# This part is inside the training loop!
        # train with real
        netD.zero_grad()
        real_cpu = data[0]
        batch_size = real_cpu.size(0)
        label = torch.full((batch_size,), real_label,
                           dtype=real_cpu.dtype)

        output = netD(real_cpu)
        errD_real = criterion(output, label)
        errD_real.backward()
        D_x = output.mean().item()

        # train with fake
        noise = torch.randn(batch_size, nz, 1, 1,)
        fake = netG(noise)
        label.fill_(fake_label)
        output = netD(fake.detach())
        errD_fake = criterion(output, label)
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        errD = errD_real + errD_fake
        optimizerD.step()
```

To train the generator, we compute the error by incompatibility between characterstics of real image and fake image as identified by the discriminator. Such that the generator can use this discrepancy measure to better fool the discriminator.

```python
# This part is inside the training loop!
        netG.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost
        output = netD(fake)
        errG = criterion(output, label)
        errG.backward()
        D_G_z2 = output.mean().item()
        optimizerG.step()
```
-->


### Implémentation de réseaux antagonistes génératifs convolutifs profonds (DCGANs)

Voir [ici](https://github.com/pytorch/examples/tree/master/dcgan) pour le code complet.

Le générateur suréchantillonne l'entrée en utilisant plusieurs modules ```nn.ConvTranspose2d``` pour produire une image à partir d'un vecteur aléatoire `nz` (bruit).

```python
class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # l'entrée est Z, elle va dans une convolution
            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),
            # la taille de l'état. (ngf*8) x 4 x 4
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # la taille de l'état. (ngf*4) x 8 x 8
            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # la taille de l'état. (ngf*2) x 16 x 16
            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            # la taille de l'état. (ngf) x 32 x 32
            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (nc) x 64 x 64
        )

    def forward(self, input):
        if input.is_cuda and self.ngpu > 1:
            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))
        else:
            output = self.main(input)
        return output

```

Le discriminateur est essentiellement un classifieur d'images qui utilise ```nn.Sigmoid()``` pour classer l'entrée comme vraie/fausse.

```python
class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.main = nn.Sequential(
            # l'entrée est (nc) x 64 x 64
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # la taille de l'état. (ndf) x 32 x 32
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # la taille de l'état. (ndf*2) x 16 x 16
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # la taille de l'état. (ndf*4) x 8 x 8
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # la taille de l'état. (ndf*8) x 4 x 4
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        output = self.main(input)
        return output.view(-1, 1).squeeze(1)
```

Nous utilisons l'entropie croisée binaire pour entraîner les réseaux.
```python
criterion = nn.BCELoss()
```

Nous avons deux optimiseurs pour chaque réseau. 
Nous voulons augmenter l'énergie des mauvais échantillons (images fausses reconnaissables) et diminuer l'énergie des bons échantillons (images réelles).

```python
optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))
```

Pour l'entraînement, nous entraînons d'abord le discriminateur avec des images réelles et des étiquettes indiquant que les images sont réelles. 
Ensuite, nous générons de fausses images à partir de bruit. 
Le discriminateur est à nouveau entraîné, mais cette fois avec des images fausses et des étiquettes indiquant qu'elles sont fausses.

```python
# Cette partie est à l'intérieur de la boucle d'entraînement !
        # entraîner avec les vraies
        netD.zero_grad()
        real_cpu = data[0]
        batch_size = real_cpu.size(0)
        label = torch.full((batch_size,), real_label,
                           dtype=real_cpu.dtype)

        output = netD(real_cpu)
        errD_real = criterion(output, label)
        errD_real.backward()
        D_x = output.mean().item()

        # entraîner avec les fausses
        noise = torch.randn(batch_size, nz, 1, 1,)
        fake = netG(noise)
        label.fill_(fake_label)
        output = netD(fake.detach())
        errD_fake = criterion(output, label)
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        errD = errD_real + errD_fake
        optimizerD.step()
```

Pour entraîner le générateur, nous calculons l'erreur par incompatibilité entre les caractéristiques de l'image réelle et de l'image fausse telles qu'identifiées par le discriminateur.
De sorte que le générateur puisse utiliser cette mesure d'incompatibilité pour mieux tromper le discriminateur.

```python
# Cette partie est à l'intérieur de la boucle d'entraînement !
        netG.zero_grad()
        label.fill_(real_label)  #les faux labels sont réels pour le coût du générateur
        output = netD(fake)
        errG = criterion(output, label)
        errG.backward()
        D_G_z2 = output.mean().item()
        optimizerG.step()
```
