---
lang: fr
lang-ref: ch.12-1
lecturer: Marc'Aurelio Ranzato
title: Traduction automatique à faibles ressources I
authors: Fanzeng Xia, Rohith Mukku
date: 21 April 2021
typora-root-url: 12-1
translation-date: 23 Jun 2021
translator: Loïck Bourdois
---

<!--
## Neural Machine Translation (NMT)
Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Its architecture typically consists of two parts, one to consume the input text sequence (encoder) and one to generate translated output text (decoder). NMT is often accompanied by an attention mechanism which helps it cope effectively with long input sequences. The decoder learns to (soft) align via attention. A translation example from Italian to English is shown in the figure below:

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure1.png" style="background-color:#DCDCDC;"><br>
<b>Figure 1:</b> A translation example from Italian to English
</center>
-->

## Traduction automatique neuronale
La traduction automatique neuronale (NMT pour *Neural Machine Translation*) est une approche d'apprentissage de bout en bout pour la tâche de traduction ayant le potentiel de surmonter bon nombre des faiblesses des systèmes de traduction conventionnels basés sur les phrases. Son architecture se compose généralement de deux parties, l'une prenant la séquence de texte d'entrée (encodeur) et l'autre pour générer le texte de sortie traduit (décodeur). La NMT est souvent accompagnée d'un mécanisme d'attention qui l'aide à faire face efficacement aux longues séquences d'entrée. Le décodeur apprend à s'aligner (en douceur) par le biais de l'attention. Un exemple de traduction de l'italien vers l'anglais est présenté dans la figure ci-dessous :

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure1.png" style="background-color:#DCDCDC;"><br>
<b>Figure 1 :</b> Un exemple de traduction de l'italien vers l'anglais
</center>

<!--
### Parallel Dataset
A parallel dataset contains a collection of original texts in language L1 and their translations into languages L2 (or consist of texts of more than two languages). As shown in the figure below, it can be used as labelled data to train NMT systems.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure2.png" style="background-color:#DCDCDC;"><br>
<b>Figure 2:</b> A parallel dataset consist of Italian and English
</center>
-->
### Jeux de données parallèles
Un jeu de données parallèles contient une collection de textes originaux dans une langue L1 et leurs traductions dans une langue L2 (ou consiste en des textes de plus de deux langues). Comme le montre la figure ci-dessous, il peut être utilisé comme données étiquetées pour entraîner les systèmes de NMT.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure2.png" style="background-color:#DCDCDC;"><br>
<b>Figure 2 :</b> Un ensemble de données parallèles composé d'italien et d'anglais
</center>


<!--
### Train NMT
The standard way of training NMT is by using maximum likelihood. Give a source sentence $\vx$ and the model parameters $\vtheta$, we will seek to maximize the log likelihood of the joint probability of all the ordered sequence of tokens in the target sentence $\vy$, as illustrated by the equation below:

$$ - \log\ p( \vy \mid \vx; \vtheta) = - \sum_{j=1}^{n} \log\ p(\vy_j \mid \vy_{j-1}, \ldots \vy_1,\vx; \vtheta) $$

It is used as a score to indicate how likely the target sentence acutally to be a translation from the source sentence. A minus sign is added in front of the equation to turn this into a minimization problem.
-->

### Entraîner la NMT
La méthode standard d'entraînement de la NMT consiste à utiliser le maximum de vraisemblance. Étant donné une phrase source $\vx$ et les paramètres du modèle $\vtheta$, nous chercherons à maximiser la vraisemblance logarithmique de la probabilité conjointe de toutes les séquences ordonnées de *tokens* dans la phrase cible $\vy$. L'équation ci-dessous l’illustre :

$$ - \log\ p( \vy \mid \vx; \vtheta) = - \sum_{j=1}^{n} \log\ p(\vy_j \mid \vy_{j-1}, \ldots \vy_1,\vx; \vtheta) $$

C’est utilisé comme un score pour indiquer la probabilité que la phrase cible soit réellement une traduction de la phrase source. Un signe moins est ajouté devant l'équation pour transformer celle-ci en un problème de minimisation.


<!--
#### Language Model
In language modelling, given a token $\vx_t$ at time $t$, we are trying to predict the next word $\vy_t$. As shown in the figure below, hidden state $\vz_t$ is generated by a RNN/CNN/Transformer block depending on the input vector $\vx_t$ and the previous hidden state $\vz_{t-1}$.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure3.png" style="background-color:#DCDCDC;"><br>
<b>Figure 3:</b> Language Model
</center>
-->

#### Modèle de langage
Dans la modélisation du langage, étant donné un *token* $\vx_t$ au temps $t$, nous essayons de prédire le mot suivant $\vy_t$. Comme le montre la figure ci-dessous, l'état caché $\vz_t$ est généré par un bloc RNN/ConvNet/Transformer en fonction du vecteur d'entrée $\vx_t$ et de l'état caché précédent $\vz_{t-1}$.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure3.png" style="background-color:#DCDCDC;"><br>
<b>Figure 3 :</b> Modèle de langage
</center>


<!--
#### Encoder-Decoder Architecture (Seq2Seq)
Given a parallel dataset (with source sentences and target sentences), we can make use of the source sentences to train a seq2seq model.

Step 1: Represent source
Represent each word in the source sentence as embeddings via source encoder.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure4.png" style="background-color:#DCDCDC;"><br>
<b>Figure 4:</b> Represent source
</center>

<br/>

Step 2: Score each source word (attention)
Take the dot product between target hidden representation $\vz_{t+1}$ and all the embeddings from source sentence. Then use softmax to get a distribution over source tokens (attention).

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure5.png" style="background-color:#DCDCDC;"><br>
<b>Figure 5:</b> Score each source word
</center>

<br/>

Step 3: Combine target hidden with source vector
Take weighted sum (weights are in the attention score vector) of the source embeddings and combine it with target hidden representation $\vz_{t+1}$. After final transformation $G(\vz)$, we get a distribution $\vy_{t+1}$ over the next word.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure6.png" style="background-color:#DCDCDC;"><br>
<b>Figure 6:</b> Combine target hidden with source vector
</center>

<br/>

Alignment is learnt implicitly in seq2seq model. All tokens can be processed in parallel efficiently with CNNs or Transformers.
-->
#### Architecture encodeur-décodeur (Seq2Seq)
Étant donné un jeu de données parallèles (avec des phrases sources et des phrases cibles), nous pouvons utiliser les phrases sources pour entraîner un modèle seq2seq.

Étape 1 : Représentation de la source
Représenter chaque mot de la phrase source en tant qu'enchâssement via l'encodeur source.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure4.png" style="background-color:#DCDCDC;"><br>
<b>Figure 4 :</b> Représenter la source
</center>

<br/>

Etape 2 : Attribuer un score à chaque mot source (attention)
Prendre le produit scalaire entre la représentation cachée cible $\vz_{t+1}$ et tous les enchâssements de la phrase source. Puis utiliser la fonction SoftMax pour obtenir une distribution sur les *tokens* source (attention).

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure5.png" style="background-color:#DCDCDC;"><br>
<b>Figure 5 :</b> Attribuer un score chaque mot source
</center>

<br/>

Étape 3 : combiner la cible cachée avec le vecteur source.
Prendre la somme pondérée (les poids sont dans le vecteur de score d'attention) des enchâssements source et la combiner avec la représentation cachée cible $\vz_{t+1}$. Après transformation finale $G(\vz)$, on obtient une distribution $\vy_{t+1}$ sur le mot suivant.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure6.png" style="background-color:#DCDCDC;"><br>
<b>Figure 6 :</b> Combiner la cible cachée avec le vecteur source
</center>

<br/>

L'alignement est appris implicitement dans le modèle seq2seq. Tous les *tokens* peuvent être traités en parallèle efficacement avec des ConvNets ou des *transformers*.


<!--
### Test NMT
After we have the translation model, and know how good they are doing on the training set, we will want to test to see how the model will actually generate translations from the source sentences.

Beam Decoding is used to search in the space of $\vy$ according to

$$ {\operatorname{argmax}} \log p(\vy \mid \vx;\vtheta) $$

Each potential choice (the number of words in vocabulary) has a probability score. At every step, beam search selects the top $k$ scoring among all the branches (maintain a queue with $k$ top scoring paths), then expand each of them and retain the top $k$ scoring paths. As illustrated by the example below:

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure7.png" style="background-color:#DCDCDC;"><br>
<b>Figure 7:</b> Beam Search
</center>

Let $k = 2$
1. Start from symbol /s, pick the top 2 scoring from all the possible choices (Life, Today)
2. Continue from Life and Today, expand each of them and calculate the path score back to the start symbol, pick the top 2 scoring path ("Like was" and "Like is")
3. So on and so for, keep proceeding until hit the end of the sentence
4. At the very last step, select the highest scoring path

Beam search is a greedy procedure and it is very effective in practice. There is a trade-off between computational cost and approximation error (the larger the value of $k$, the better the approximation and the higher the computational cost).

Issues of Beam Search:
Beam search will always select the highest scoring path. Thus, the solution will be biased because of such strategy. It doesn't handle uncertainty well.

Other decoding methods: sampling, top-$k$ sampling, generative and discriminative reranking
-->

### Tester notre NMT
Une fois que nous disposons du modèle de traduction et que nous connaissons son efficacité sur l'ensemble d'apprentissage, nous souhaitons le tester pour voir comment le modèle va réellement générer des traductions à partir des phrases sources.

Le décodage en faisceau est utilisé pour rechercher dans l'espace des $\vy$ selon la formule suivante :

$$ {\operatorname{argmax}} \log p(\vy \mid \vx;\vtheta) $$

Chaque choix potentiel (le nombre de mots dans le vocabulaire) a un score de probabilité. À chaque étape, la recherche en faisceau (*beam search* en anglais) sélectionne les $k$ meilleurs scores parmi toutes les branches (maintient une file d'attente avec $k$ chemins les mieux notés), puis développe chacun d'entre eux et conserve les $k$ meilleurs scores. Comme l'illustre l'exemple ci-dessous :

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure7.png" style="background-color:#DCDCDC;"><br>
<b>Figure 7 :</b> Recherche en faisceau
</center>

Soit $k = 2$
1. Partant du symbole */s*, elle choisit les 2 meilleurs scores parmi tous les choix possibles (*Vie*, *Aujourd'hui*).
2. On continue à partir de *Vie* et *Aujourd'hui* et on développe chacun de ces deux mots. On calcule le score du chemin de retour au symbole de départ. On choisit les 2 chemins les mieux notés (*Vie est* et *Vie était*).
3. Et ainsi de suite, jusqu'à atteindre la fin de la phrase.
4. À la toute dernière étape, on sélectionne le chemin le plus performant.

La recherche en faisceau est une procédure avide et très efficace en pratique. Il existe un compromis entre le coût de calcul et l'erreur d'approximation (plus la valeur de $k$ est grande, meilleure est l'approximation et plus élevé est le coût de calcul).

Problèmes de la recherche en faisceau :
La recherche en faisceau sélectionnera toujours le chemin ayant le score le plus élevé. Ainsi, la solution sera biaisée à cause de cette stratégie. Elle ne gère pas bien l'incertitude.

Autres méthodes de décodage : échantillonnage, échantillonnage top-$k$, reclassement génératif et discriminatif.

<!--
### NMT Training & Inference Summary

Training: predict one target token at the time and minimize cross-entropy loss

Inference: find the most likely target sentence (approximately) using beam search

Evaluation: compute BLEU on hypothesis returned by the inference procedure

We first compute the geometric average of the modified n-gram precisions, $p_n$

$$p_n = \frac{\sum_{\text{generated sentences}} \sum_{\text{ngrams}} \text{Clip(Count(ngram matches))}}{\sum_{\text{generated sentences}} \sum_{\text{ngrams}} \text{Count(ngram)}}$$

Candidate translations longer than their references are already penalized by the modified n-gram precision measure: there is no need to penalize them again. Consequently, we introduce a multiplicative brevity penalty factor, BP (denoted as $$f_{\text{BP}}$$ in the formula). With BP in place, a high-scoring candidate translation must now match the reference translations in length, in word choice, and in word order.

$$ s_\text{BLEU} = f_{\text{BP}} \exp\left({\sum_{n=1}^{N} \frac{1}{N} \log \ p_n}\right) $$

In brief, BLEU score (denoted as $s_{\text{BLEU}}$ in the formula) measures the similarity between the translation generated by the model and a reference translation created by a human.
-->

### Résumé de l’entraînement et de l'inférence d’une NMT

Apprentissage : prédire une phrase cible à la fois et minimiser la perte d'entropie croisée.

Inférence : trouver la phrase cible la plus probable (approximativement) en utilisant la recherche en faisceau.

Évaluation : calcul du score BLEU sur les hypothèses retournées par la procédure d'inférence.

Nous calculons d'abord la moyenne géométrique des précisions modifiées des n-grammes, $p_n$.

$$p_n = \frac{\sum_{\text{generated sentences}} \sum_{\text{ngrams}} \text{Clip(Count(ngram matches))}}{\sum_{\text{generated sentences}} \sum_{\text{ngrams}} \text{Count(ngram)}}$$

Les traductions candidates plus longues que leurs références sont déjà pénalisées par la mesure de précision n-gram modifiée Il n'est pas nécessaire de les pénaliser à nouveau. Par conséquent, nous introduisons un facteur multiplicatif de pénalité de brièveté, BP (désigné par $$f_{\text{BP}}$$ dans la formule). Avec BP en place, une traduction candidate bien notée doit maintenant correspondre aux traductions de référence en termes de longueur, de choix de mots et d'ordre des mots.

$$ s_\text{BLEU} = f_{\text{BP}} \exp\left({\sum_{n=1}^{N} \frac{1}{N} \log \ p_n}\right) $$

En bref, le score BLEU (désigné par $s_{\text{BLEU}}$ dans la formule) mesure la similarité entre la traduction générée par le modèle et une traduction de référence créée par un humain.

<!--
## Machine Translation in Other Languages

The above theory has 2 assumptions:
1. The languages that we considered are English and Italian, both of which are European languages with some commonality.
2. We have a lot of data because, in general, we need three data points to estimate a parameter and these models have hundreds of millions of parameters.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure8.png" style="background-color:#DCDCDC;"><br>
<b>Figure 8:</b> Statistics of the languages spoken in the world, tail represents the huge number of languages with not much speakers
</center>

If we consider the statistics, there are more than 6000 languages in the world and not more than 5% of the world population speaks native English. There are a number of languages for which we don't have much data available and Google Translate performs poorly. The other issue with these languages is that they are mostly spoken and not written.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure9.png" style="background-color:#DCDCDC;"><br>
<b>Figure 9:</b> Translation quality degrades rapidly for low resource languages
</center>
-->

## Traduction automatique dans d'autres langues

La théorie ci-dessus repose sur deux hypothèses :
1. Les langues que nous avons considérées sont l'anglais et l'italien, qui sont toutes deux des langues européennes ayant des points communs.
2. Nous avons beaucoup de données car, en général, nous avons besoin de trois points de données pour estimer un paramètre et ces modèles ont des centaines de millions de paramètres.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure8.png" style="background-color:#DCDCDC;"><br>
<b>Figure 8 :</b> Statistiques des langues parlées dans le monde, la queue représente le nombre énorme de langues avec peu de locuteurs
</center>

Si l'on considère les statistiques, il y a plus de 6000 langues dans le monde et seuelement 5% de la population mondiale a l'anglais comme langue maternelle. Il y a un certain nombre de langues pour lesquelles nous n'avons pas beaucoup de données disponibles et les performances de Google Translate sont faibles. L'autre problème de ces langues est qu'elles sont surtout parlées et non écrites.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure9.png" style="background-color:#DCDCDC;"><br>
<b>Figure 9 :</b> La qualité de la traduction se dégrade rapidement pour les langues à faibles ressources
</center>

<!--
### Machine Translation in Practice
Let's consider an English to Nepali Machine Translation system where we translate English news to Nepali. Nepali is a low resource language and the amount of parallel data for training is very small.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure10.png" style="background-color:#DCDCDC;"><br>
<b>Figure 10:</b> Open Parallel Corpus Dataset resources for English to Nepali
</center>

The open parallel corpus is a place where we can get these datasets. There are 2 issues here:
1. Now, if we look for Nepali dataset, we see that either there is very less amount of quality data set or huge dataset with no useful content.
2. There are possibilities that the source and target data are not parallel. We might have more data in English in some categories, more data in Nepali in other categories. There are cases these two don't match.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure11.png" style="background-color:#DCDCDC;"><br>
<b>Figure 11:</b> Machine Translation from English to Nepali with the help of other languages like Hindi
</center>

One way to solve these issues is to make use of other languages related to Nepali. For example, Hindi is a much higher resource language and belongs to the same family as Nepali. We can extend this to other languages also.
-->

### La traduction automatique en pratique
Prenons l'exemple d'un système de traduction automatique dans lequel nous traduisons des actualités de l'anglais vers le népalais. Le népalais est une langue à faible ressource et la quantité de données parallèles pour l'entraînement est très faible.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure10.png" style="background-color:#DCDCDC;"><br>
<b>Figure 10 :</b> Ressources du jeu de données « Open Parallel Corpus » pour l'anglais vers le népalais
</center>

L’*Open Parallel Corpu* est un endroit où nous pouvons obtenir ces jeux de données. Il y a 2 problèmes ici :
1. si nous cherchons un jeu de données de données en népalais, nous voyons que soit il y en a très peu de qualité, soit un grand sans contenu utile.
2. il est possible que les données source et cible ne soient pas parallèles. Nous pouvons avoir plus de données en anglais dans certaines catégories, plus de données en népalais dans d'autres catégories. Il y a des cas où ces deux ne correspondent pas.

Une façon de résoudre ces problèmes est de faire appel à d'autres langues liées au népalais. Par exemple, l'hindi est une langue aux ressources beaucoup plus élevées et appartient à la même famille que le népalais. Nous pouvons étendre cela à d'autres langues également.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure11.png" style="background-color:#DCDCDC;"><br>
<b>Figure 11 :</b> Traduction automatique de l'anglais au népalais avec l'aide d'autres langues comme l'hindi
</center>


<!--
### Low Resource Machine Translation

In practice, whenever a model is being trained, there is a lot of effort that goes in analyzing the model and the properties of the data. This whole thing is an iterative process and the machine learning practitioner needs to view this with a complete picture. Secondly, if there is less data, we can downscale the model, which isn't desirable. We need to come up with ways to enlarge the dataset somehow or use unsupervised techniques.

| <center><img src="{{site.baseurl}}/images/week12/12-1/figure12.png"/></center> | <center><img src="{{site.baseurl}}/images/week12/12-1/figure13.png"/></center> |
|                              (a) Data collection in ML                         |                              (b) Low-resource tasks in ML                      |

<center>
<b>Figures 12, 13:</b> Data collection and Low-resource tasks in ML
</center>
-->

### Traduction automatique à faibles ressources

En pratique, chaque fois qu'un modèle est entraîné, de nombreux efforts sont déployés pour analyser le modèle et les propriétés des données. Il s'agit d'un processus itératif nécessitant d’avoir une vue d'ensemble de la situation. Deuxièmement, s'il y a moins de données, nous pouvons réduire l'échelle du modèle, ce qui n'est pas souhaitable. Nous devons trouver des moyens d'élargir le jeu de données d'une manière ou d'une autre ou utiliser des techniques non supervisées.

| <center><img src="{{site.baseurl}}/images/week12/12-1/figure12.png"/></center> | <center><img src="{{site.baseurl}}/images/week12/12-1/figure13.png"/></center> |
| (a) Collecte de données en apprentissage machine | (b) Tâches à faibles ressources en apprentissage machine |

<center>
<b>Figures 12 et 13 :</b> Collecte de données et tâches à faibles ressources en apprentissage machine
</center>

<!--
#### Challenges

Loose definition of Low Resource MT: A language pair can be considered **low resource** when the number of parallel sentences is in the order of 10,000 or less.

Challenges encountered in Low Resource Machine Translation tasks:
- Datasets
  * Sourcing data to train on
  * High quality evaluation datasets
- Metrics
  * Human evaluation
  * Automatic evaluation
- Modeling
  * Learning paradigm
  * Domain adaption
  * Generalization
- Scaling

Additionally, there will be challenges that are encountered in general Machine Translation tasks:
- Exposure bias (training for generation)
- Modeling uncertainty
- Automatic evaluation
- Budget computation
- Modeling the tails
- Efficiency
-->

#### Défis

Définition approximative de la traduction automatique à faibles ressources : une paire de langues peut être considérée comme **à faibles ressources** lorsque le nombre de phrases parallèles est de l'ordre de 10 000 ou moins.

Défis rencontrés dans les tâches de traduction automatique à faibles ressources :
- Jeux de données :
  * Trouver des données pour l'entraînement
  * Trouver des données d'évaluation de haute qualité
- Métrique :
  * Évaluation humaine
  * Évaluation automatique
- Modélisation :
  * Paradigme d'apprentissage
  * Adaptation au domaine
  * Généralisation
- Mise à l'échelle

En outre, il y a des défis qui sont rencontrés dans les tâches générales de traduction automatique :
- Biais d'exposition (entraînement pour la génération)
- Modélisation de l'incertitude
- Évaluation automatique
- Calcul du budget
- Modélisation des queues
- Efficacité


<!--
### MAD Cycle of Research

There are 3 pillars in the cycle of research:
1. Data: We collect data that we want to explore.
2. Model: We feed the collected data and use the data distribution, develop algorithms and build models.
3. Analysis: After we get the model, we test the model by checking how well it fits with the data distribution or how well it performs using different metrics.

This process is iterated several times to get a good a performance.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure14.png" style="background-color:#DCDCDC;"><br>
<b>Figure 14:</b> 3 Pillars in the cycle of research
</center>

The highlighted examples in the Figure 14 will be discussed more in detail in the following sections.
-->

### Cycle de recherche MAD

Le cycle de recherche repose sur 3 piliers :
1. Données : nous collectons les données que nous voulons explorer.
2. Modèle : nous fournissons les données collectées et utilisons la distribution des données, nous développons des algorithmes et construisons des modèles.
3. Analyse : après avoir obtenu le modèle, nous le testons en vérifiant son adéquation avec la distribution des données ou ses performances à l'aide de différentes métriques.

Ce processus est itéré plusieurs fois pour obtenir une bonne a performance.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure14.png" style="background-color:#DCDCDC;"><br>
<b>Figure 14 :</b> 3 piliers dans le cycle de la recherche.
</center>

Les exemples mis en évidence dans la figure 14 seront discutés plus en détail dans les sections suivantes.


<!--
#### Data

Going back to the same example of English-Nepali news translation, we collect data belonging to different domains. Here, dataset of Bible and GNOME, Ubuntu won't be of much help for news translation.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure15.png" style="background-color:#DCDCDC;"><br>
<b>Figure 15:</b> English-Nepali dataset
</center>

The question here is *how can we evaluate the part of dataset that is not there on the right side (Nepali)?* This lead to creation of **FLoRes Evaluation Benchmark**. It contains texts (taken from Wikipedia documents) from Nepali, Sinhala, Khmer, Pashto.

Note: *These are not parallel dataset*

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure16.png" style="background-color:#DCDCDC;"><br>
<b>Figure 16:</b> Flores Data Collection Process
</center>

These sentences are translated and the quality of the translations are determined using the automatic checks and human evaluation. There are several techniques in automatic checks, a model was trained on each language and perplexity was measured. If the perplexity is too high, then we go back to translation stage as shown above. Other checks are transliteration, using Google Translate, etc. There is no criteria or conditions for when this loop is stopped, essentially it depends on the perplexity and any outliers.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure17.png" style="background-color:#DCDCDC;"><br>
<b>Figure 17:</b> Some examples of English Sinhala translations
</center>

Figure 17 shows some examples. As we can see, they are not really fluent. One more issue with these is that the wikipedia articles of Sinhala language mostly contain topics of very limited number of domains (like their religion, country, etc.)
-->

#### Données

Pour revenir à l'exemple de traduction d’actualités en anglais-népalais, nous collectons des données appartenant à différents domaines. Ici, le jeu de données de la Bible, de GNOME/Ubuntu ne seront pas d'une grande aide pour la traduction d’actualités.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure15.png" style="background-color:#DCDCDC;"><br>
<b>Figure 15 :</b> Jeux de données anglais-népalais
</center>

La question ici est : comment pouvons-nous évaluer la partie de l'ensemble de données absente sur le côté droit (népalais) ?  
Cette question a conduit à la création du **FLoRes Evaluation Benchmark**. Il contient des textes (tirés d'articles de Wikipédia) en népalais, singhalais, khmer et pachto. **Il ne s'agit pas de jeux de données parallèles**.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure16.png" style="background-color:#DCDCDC ;"><br>
<b>Figure 16 :</b> Processus de collecte des données de Flores
</center>

Ces phrases sont traduites et la qualité des traductions est déterminée à l'aide de contrôles automatiques et d'évaluations humaines. Il existe plusieurs techniques dans les vérifications automatiques. Un modèle a été entraîné sur chaque langue et la perplexité a été mesurée. Si la perplexité est trop élevée, nous revenons à l’étape de traduction comme indiqué ci-dessus. D'autres vérifications sont la translittération, l'utilisation de Google Translate, etc. Il n'y a pas de critères ou de conditions pour savoir quand cette boucle doit s'arrêter, cela dépend essentiellement de la perplexité et des éventuelles valeurs aberrantes.

<center>
<img src="{{site.baseurl}}/images/week12/12-1/figure17.png" style="background-color:#DCDCDC;"><br>
<b>Figure 17 :</b> Quelques exemples de traductions anglaises-singhalaises
</center>

La figure 17 montre quelques exemples. Comme on peut le constater, elles ne sont pas vraiment fluides. Un autre problème est que les articles Wikipedia de la langue singhalaise contiennent surtout des sujets d'un nombre très limité de domaines (comme la religion, l'histoire du pays, etc.).
