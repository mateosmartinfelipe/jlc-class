---
lang: fr
lang-ref: ch.12-3
title: Commande à modèle prédictif (version EBM)
lecturer: Alfredo Canziani
authors: Yang Zhou, Daniel Yao
date: 28 Apr 2021
typora-root-url: 12-3
translation-date: 20 Jun 2021
translator: Loïck Bourdois
---    


<!--
## Action plan
- Model predictive control **[Here we are today]**
    - Backprop through kinematic equation
    - Minimisation of the latent
- Truck backer-upper
    - Learning an emulator of the kinematics from observations
    - Training a policy
- PPUU
    - Stochastic environment
    - Uncertainty minimisation
    - Latent decoupling
-->

## Plan d'action
- Commande à modèle prédictif **[Nous sommes ici dans cette section]**.
    - Rétropropagation par l'équation cinématique
    - Minimisation de la latence
- Truck backer-upper
    - Apprentissage d'un émulateur de la cinématique à partir d'observations
    - Entraînement d'une politique
- Prédiction ét apprentissage d'une politique sous incertitude (PPUU de l'anglais « Prediction and Policy learning Under Uncertainty »)
    - Environnement stochastique
    - Minimisation des incertitudes
    - Découplage latent
 
<!--
## State transition equations -- Evolution of the state

Here we discuss a state transition equation where $\vx$ represents the state, $\vu$ represents control. We can formulate the state transition function in a continuous-time system where $\vx(t)$ is a function of continuous variable $t$.

<div class="MathJax_Display" style="text-align: center;">
$$
\begin{aligned}
\dot{\vx} &= f(\vx,\vu)\\
\frac{\partial \vx(t)}{\partial t} &= f(\vx(t), \vu(t))
\end{aligned}
$$
</div>

<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure1.png" style="background-color:#DCDCDC;" /><br>
Figure 1: State and control illustration of a three-cycle
</center>

We use a tri-cycle as the example to study it. The orange wheel is the control $\vu$, $(x_c,y_c)$ is the instantaneous center of rotation. You can also have two wheels in the front. For simplicity, we use one wheel as the example.

In this example $\vx=(x,y,\theta,s)$ is the state, $\vu=(\phi,\alpha)$ is the control.

$$
\left\{\begin{array}{l}
\dot{x}=s \cos \theta \\
\dot{y}=s \sin \theta \\
\dot{\theta}=\frac{s}{L} \tan \phi \\
\dot{s}=a
\end{array}\right.
$$


We can reformulate the differential equation from continuous-time system to discreate-time system

$$
\vx[t]=\vx[t-1]+f(\vx[t-1], \vu[t]) \mathrm{d} t
$$

To be clear, we show the units of $\vx, \vu$.

$$
\begin{array}{l}
{[\vu]=\left(\mathrm{rad}\  \frac{\mathrm{m}}{\mathrm{s}^{2}}\right)} \\
{[\vx]=\left(\mathrm{m} \  \mathrm{m} \  \mathrm{rad} \  \frac{\mathrm{m}}{\mathrm{s}}\right)}
\end{array}
$$

Let's take a look at different examples. We use different color for variables we care about.

<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure2.png" style="background-color:#DCDCDC;" /><br>
Figure 2: State Formulation
</center>

Example 1: Uniform Linear Motion: No acceleration, no steering
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure3.svg" style="background-color:#DCDCDC;" /><br>
Figure 3: Control of Uniform Linear Motion
</center>
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure4.svg" style="background-color:#DCDCDC;" /><br>
Figure 4: State of Uniform Linear Motion
</center>


Example 2: Crush into itself: Negative acceleration, no steering
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure5.svg" style="background-color:#DCDCDC;" /><br>
Figure 5: Control of Curshing into itself
</center>
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure6.svg" style="background-color:#DCDCDC;" /><br>
Figure 6: State of Curshing into itself
</center>


Example 3: Sine wave: Positive steering for the first part, negative steering for the second part
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure7.svg" style="background-color:#DCDCDC;" /><br>
Figure 7: Control of Sine Wave
</center>
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure8.svg" style="background-color:#DCDCDC;" /><br>
Figure 8: State of Sine Wave
</center>
-->

## Equations de transition d'état - Evolution de l'état

Nous abordons ici une équation de transition d'état où $\vx$ représente l'état et $\vu$ représente la commande. 
Nous pouvons formuler la fonction de transition d'état dans un système à temps continu où $\vx(t)$ est une fonction de la variable continue $t$.

<div class="MathJax_Display" style="text-align: center;">
$$
\begin{aligned}
\dot{\vx} &= f(\vx,\vu)\\
\frac{\partial \vx(t)}{\partial t} &= f(\vx(t), \vu(t))
\end{aligned}
$$
</div>

<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure1.png" style="background-color:#DCDCDC;" /><br>
<b> Figure 1 :</b> Illustration de l'état et de la commande d'un tricycle
</center>

Nous utilisons un tricycle comme exemple pour l'étudier. La roue orange est la commande $\vu$, $(x_c,y_c)$ est le centre de rotation instantané.
On peut aussi avoir deux roues à l'avant. Pour simplifier, nous utilisons une roue comme exemple.

Dans cet exemple, $\vx=(x,y,\theta,s)$ est l'état, $\vu=(\phi,\alpha)$ est la commande.

$$
\left\{\begin{array}{l}
\dot{x}=s \cos \theta \\
\dot{y}=s \sin \theta \\
\dot{\theta}=\frac{s}{L} \tan \phi \\
\dot{s}=a
\end{array}\right.
$$


Nous pouvons reformuler l'équation différentielle du système à temps continu en système à temps discret :

$$
\vx[t]=\vx[t-1]+f(\vx[t-1], \vu[t]) \mathrm{d} t
$$

Pour être clair, nous montrons les unités de $\vx, \vu$.

$$
\begin{array}{l}
{[\vu]=\left(\mathrm{rad}\  \frac{\mathrm{m}}{\mathrm{s}^{2}}\right)} \\
{[\vx]=\left(\mathrm{m} \  \mathrm{m} \  \mathrm{rad} \  \frac{\mathrm{m}}{\mathrm{s}}\right)}
\end{array}
$$

Jetons un coup d'oeil à différents exemples. Nous utilisons une couleur différente pour les variables qui nous intéressent.

<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure2.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 2 :</b> Formulation de l'état
</center>
<br>

Exemple 1 : Mouvement linéaire uniforme : pas d'accélération, pas de direction.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure3.svg" style="background-color:#DCDCDC;" /><br>
<b>Figure 3 :</b> Contrôle d'un mouvement linéaire uniforme
</center>

<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure5.svg" style="background-color:#DCDCDC;" /><br>
<b>Figure 4 :</b> Etat d'un mouvement linéaire uniforme
</center>
<br>
<br>

Exemple 2 : Ecrasement sur lui-même : accélération négative, pas de direction.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure5.svg" style="background-color:#DCDCDC;" /><br>
<b>Figure 5 :</b> Contrôle de l'incurvation sur elle-même
</center>
<br>

<center>
<img src= "{{site.baseurl}}/images/week12/12-3/figure6.svg" style="background-color:#DCDC;" /><br>
<b>Figure 6 :</b> État de l'incurvation sur elle-même
</center>
<br>
<br>

Exemple 3 : Onde sinusoïdale : direction positive pour la première partie, direction négative pour la deuxième partie.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure7.svg" style="background-color:#DCDCDC;" /><br>
<b>Figure 7 :</b> Contrôle de l'onde sinusoïdale
</center>
<br>

<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure8.svg" style="background-color:#DCDC;" /><br>
<b>Figure 8 :</b> Etat de l'onde sinusoïdale
</center>



<!--
## Kelley-Bryson algorithm
What if we want the tri-cycle to reach a specified destination with a specified speed?
- This can be achieved by inference using **Kelley-Bryson algorithm**, which utilizes **backprop through time** and **gradient descent**.
-->
## Algorithme de Kelley-Bryson
Que faire si l'on veut que le tricycle atteigne une destination donnée à une vitesse donnée ?  
Cela peut être réalisé par inférence à l'aide de l'algorithme de **Kelley-Bryson**, qui utilise la **rétropropation à travers le temps** et la **descente de gradient**.


<!--
### Recap of RNN
We can compare the inference process here  with the training process of RNN.

Below is an RNN schematic chart. We feed variable $\vx[t]$ and the previous state $\vh[t-1]$ into the predictor, while $\vh[0]$ is set to be zero. The predictor outputs the hidden representation $\vh[t]$.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure9.png" style="background-color:#DCDCDC;" /><br>
Figure 9: RNN schematic chart
</center>
-->
### Récapitulation des réseaux de neurones récurrents (RNNs)
Nous pouvons comparer le processus d'inférence ici avec le processus d'entraînements des RNNs.

Voici le schéma d'un RNN. Nous introduisons la variable $\vx[t]$ et l'état précédent $\vh[t-1]$ dans le prédicteur, tandis que $\vh[0]$ est fixé à zéro. 
Le prédicteur sort la représentation cachée $\vh[t]$.

<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure9.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 9 :</b> Schéma d'un RNN
</center>

<!--
### Optimal control (inference)
In optimal control (inference) shown as below, we feed the latent variable (control) $\vz[t]$ and the previous state $\vx[t-1]$ into the predictor, while $\vx[0]$ is set to be $\vx_0$. The predictor outputs the state $\vx[t]$.

<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure10.png" style="background-color:#DCDCDC;" /><br>
Figure 10: Optimal Control schematic chart
</center>

Backprop is implemented in both RNN and Optimal Control. However, gradient descent is implemented with respect to predictor's parameters in RNN, and is implemented wrt latent variable $\vz$ in optimal control.
-->

### Contrôle optimal (inférence)
Dans le contrôle optimal (inférence) présenté ci-dessous, nous introduisons la variable latente (contrôle) $\vz[t]$ et l'état précédent $\vx[t-1]$ dans le prédicteur, tandis que $\vx[0]$ est fixé à $\vx_0$. Le prédicteur sort l'état $\vx[t]$.

<center>
<img src= "{{site.baseurl}}/images/week12/12-3/figure10.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 10 :</b> Schéma de la commande optimale
</center>

La rétropropagation est implémentée à la fois dans le RNN et dans le contrôle optimal. 
Cependant, la descente de gradient est implémentée par rapport aux paramètres du prédicteur dans le RNN et est implémentée par rapport à la variable latente $\vz$ dans le contrôle optimal.


<!--
### Unfolded version of optimal control
In unfolded version of optimal control, cost can be set to either the final step of the tri-cycle or every step of the tri-cycle. Besides, cost functions can take many forms, such as Average Distance, Softmin, etc.


#### Set the cost to the final step
From the figure below, we can see there is only one cost $c$ set in the final step (step 5), which measures the distance of our target $\vy$ and state $\vx[5]$ with control $\vz[5]$
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure11.png" style="background-color:#DCDCDC;" /><br>
Figure 11: Cost to the final step
</center>

$(1)$ If the cost function only involves the final position with no restrictions on the final speed, we can obtain the results after inference shown as below.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure12.png" style="background-color:#DCDCDC;" /><br>
Figure 12: Cost function involving only the final position
</center>
From the figure above, it is seen that when $T=5$ or $T=6$, the final position meets the target position, but when $T$ is above 6 the final position does not.

$(2)$ If the cost function involves the final position and zero final speed, we can obtain the results after inference shown as below.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure13.png" style="background-color:#DCDCDC;" /><br>
Figure 13: Cost function involving the final position and zero final speed
</center>
From the figure above, it is seen that when $T=5$ or $T=6$, the final position roughly meets the target position, but when $T$ is above 6 the final position does not.
-->

### Version dépliée du contrôle optimal
Dans la version dépliée du contrôle optimal, le coût peut être fixé soit à l'étape finale du tricycle, soit à chaque étape du tricycle. 
En outre, les fonctions de coût peuvent prendre de nombreuses formes, telles que la distance moyenne, la softmin, etc.


#### Fixer le coût à l'étape finale
Sur la figure ci-dessous, nous pouvons voir qu'il n'y a qu'un seul coût $c$ fixé à l'étape finale (étape 5) mesurant la distance de notre cible $\vy$ et de l'état $\vx[5]$ avec le contrôle $\vz[5]$.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure11.png" style="background-color:#DCDC;" /><br>
<b>Figure 11 :</b> Coût de l'étape finale
</center>
<br>
<br>

$(1)$ Si la fonction de coût ne fait intervenir que la position finale sans restriction sur la vitesse finale, nous pouvons obtenir les résultats après inférence présentés comme ci-dessous.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure12.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 12 :</b> Fonction de coût faisant intervenir uniquement la position finale.
</center>
D'après la figure ci-dessus, on constate que lorsque $T=5$ ou $T=6$, la position finale respecte la position cible. 
Mais lorsque $T$ est supérieur à 6, la position finale ne le fait pas.
<br>

$(2)$ Si la fonction de coût fait intervenir la position finale et la vitesse finale nulle, on peut obtenir les résultats après inférence présentés ci-dessous.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure13.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 13 :</b> Fonction de coût faisant intervenir la position finale et la vitesse finale nulle.
</center>
D'après la figure ci-dessus, on constate que lorsque $T=5$ ou $T=6$, la position finale correspond à peu près à la position cible. 
Mais que lorsque $T$ est supérieur à 6, la position finale ne correspond pas.


<!--
### Set the cost to every step
From the figure below, we can see there is a cost $c$ set in every step.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure14.png" style="background-color:#DCDCDC;" /><br>
Figure 14: Cost to every step
</center>

$(1)$ Cost Example: Average Distance
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure15.png" style="background-color:#DCDCDC;" /><br>
Figure 15: Cost Example: Average Distance
</center>

$(2)$ Cost Example: Softmin
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure16.png" style="background-color:#DCDCDC;" /><br>
Figure 16: Cost Example: Softmin
</center>

Different forms of cost functions can be explored through experimentation.
-->

### Définir le coût à chaque étape
D'après la figure ci-dessous, nous pouvons voir qu'il y a un coût $c$ fixé à chaque étape.
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure14.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 14 :</b> Coût de chaque étape
</center>
<br>

Exemple de coût $(1)$ : Distance moyenne
<center>
<img src= "{{site.baseurl}}/images/week12/12-3/figure15.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 15 :</b> Exemple de coût : distance moyenne
</center>

Exemple de coût de $(2)$ : Softmin
<center>
<img src="{{site.baseurl}}/images/week12/12-3/figure16.png" style="background-color:#DCDCDC;" /><br>
<b>Figure 16 :</b> Exemple de coût : softmin
</center>

Différentes formes de fonctions de coût peuvent être explorées par l'expérimentation.

<!--
## Optimization_Path_Planner-Notebook
In this notebook, we use tri-cycle as an example as well.
-->
## Notebook *Optimisation_Path_Planner*
Dans ce *notebook*, nous utilisons également le tricycle comme exemple.


<!--
### Define kinematic model of a tricycle $\dot{\vx}=f(\vx,\vu)$.
* $\vx$ represents state: ($x$, $y$, $θ$, $s$)
* $\vu$ represents control: ($ϕ$, $a$)
* We feed $\vx[t-1]$ and $\vu[t]$ to obtain the next state $\vx[t]$

```python
def f(x, u, t=None):
    L = 1  # m
    x, y, θ, s = x

    ϕ, a = u
    f = torch.zeros(4)
    f[0] = s * torch.cos(θ)
    f[1] = s * torch.sin(θ)
    f[2] = s / L * torch.tan(ϕ)
    f[3] = a
    return f
```
-->

### Définir le modèle cinématique d'un tricycle $\dot{\vx}=f(\vx,\vu)$
* $\vx$ représente l'état : ($x$, $y$, $θ$, $s$)
* $\vu$ représente le contrôle : ($ϕ$, $a$)
* On donne $\vx[t-1]$ et $\vu[t]$ pour obtenir l'état suivant $\vx[t]$

```python
def f(x, u, t=None):
    L = 1  # m
    x, y, θ, s = x

    ϕ, a = u
    f = torch.zeros(4)
    f[0] = s * torch.cos(θ)
    f[1] = s * torch.sin(θ)
    f[2] = s / L * torch.tan(ϕ)
    f[3] = a
    return f
```


<!--
### Define several cost functions
As mentioned above, cost functions can take various forms. In this notebook, we list 5 kinds as follows:
* `vanilla_cost`: Focuses on the final position
* `cost_with_target_s`: Focuses on the final position and final zero speed.
* `cost_sum_distances`: Focuses on the position of every step, and minimizes the mean of the distances.
* `cost_sum_square_distances`: Focuses on the position of every step, and minimizes the mean of squared distances.
* `cost_logsumexp`: The distance of the closest position should be minimized.


```python
def vanilla_cost(state, target):
    x_x, x_y = target
    return (state[-1][0] - x_x).pow(2) + (state[-1][1] - x_y).pow(2)

def cost_with_target_s(state, target):
    x_x, x_y = target
    return (state[-1][0] - x_x).pow(2) + (state[-1][1] - x_y).pow(2)
                                       + (state[-1][-1]).pow(2)

def cost_sum_distances(state, target):
    x_x, x_y = target
    dists = ((state[:, 0] - x_x).pow(2) + (state[:, 1] - x_y).pow(2)).pow(0.5)
    return dists.mean()

def cost_sum_square_distances(state, target):
    x_x, x_y = target
    dists = ((state[:, 0] - x_x).pow(2) + (state[:, 1] - x_y).pow(2))
    return dists.mean()

def cost_logsumexp(state, target):
    x_x, x_y = target
    dists = ((state[:, 0] - x_x).pow(2) + (state[:, 1] - x_y).pow(2))#.pow(0.5)
    return -1 * torch.logsumexp(-1 * dists, dim=0)
```
-->

### Définir plusieurs fonctions de coût
Comme mentionné ci-dessus, les fonctions de coût peuvent prendre différentes formes. Dans ce *notebook*, nous en listons 5 sortes comme suit :
* `vanilla_cost` : se concentre sur la position finale
* `cost_with_target_s` : se concentre sur la position finale et la vitesse zéro finale
* `cost_sum_distances` : se concentre sur la position de chaque étape, et minimise la moyenne des distances
* `cost_sum_square_distances` : se concentre sur la position de chaque étape, et minimise la moyenne des distances au carré
* `cost_logsumexp` : la distance de la position la plus proche doit être minimisée

```python
def vanilla_cost(state, target):
    x_x, x_y = target
    return (state[-1][0] - x_x).pow(2) + (state[-1][1] - x_y).pow(2)

def cost_with_target_s(state, target):
    x_x, x_y = target
    return (state[-1][0] - x_x).pow(2) + (state[-1][1] - x_y).pow(2)
                                       + (state[-1][-1]).pow(2)

def cost_sum_distances(state, target):
    x_x, x_y = target
    dists = ((state[:, 0] - x_x).pow(2) + (state[:, 1] - x_y).pow(2)).pow(0.5)
    return dists.mean()

def cost_sum_square_distances(state, target):
    x_x, x_y = target
    dists = ((state[:, 0] - x_x).pow(2) + (state[:, 1] - x_y).pow(2))
    return dists.mean()

def cost_logsumexp(state, target):
    x_x, x_y = target
    dists = ((state[:, 0] - x_x).pow(2) + (state[:, 1] - x_y).pow(2))#.pow(0.5)
    return -1 * torch.logsumexp(-1 * dists, dim=0)
```



<!--
### Define path planning with cost
* The optimizer is set to be SGD.
* Time interval `T` is set to be 1s.
* We need to compute every state from the initial state by the following code:
```python
x = [torch.tensor((0, 0, 0, s),dtype=torch.float32)]
for t in range(1, T+1):
      x.append(x[-1] + f(x[-1], u[t-1]) * dt)
x_t = torch.stack(x)
```
* Then compute the cost:
```python
cost = cost_f(x_t, (x_x, x_y))
costs.append(cost.item())
```
* Implement backprop and update $\vu$
```python
optimizer.zero_grad()
cost.backward()
optimizer.step()
```
* Now we can feed values to path_planning_with_cost to obtain inference results and plot trajectories. **Example**:
```python
path_planning_with_cost(
      x_x=5, x_y=1, s=1, T=5, epochs=5,
      stepsize=0.01, cost_f=vanilla_cost, debug=False
)
```
-->


### Définir la planification du chemin avec le coût
* L'optimiseur est défini comme étant la SGD.
* L'intervalle de temps `T` est fixé à 1s.
* Nous devons calculer chaque état à partir de l'état initial avec le code suivant :
* 
```python
x = [torch.tensor((0, 0, 0, s),dtype=torch.float32)]
for t in range(1, T+1):
      x.append(x[-1] + f(x[-1], u[t-1]) * dt)
x_t = torch.stack(x)
```

* Ensuite, nous calculons le coût :
```python
cost = cost_f(x_t, (x_x, x_y))
costs.append(cost.item())
```

* Implémentations de la rétropropagation et mise à jour de $\vu$.
```python
optimizer.zero_grad()
cost.backward()
optimizer.step()
```

* Maintenant, nous pouvons fournir des valeurs à `path_planning_with_cost` pour obtenir des résultats d'inférence et tracer des trajectoires. **Exemple** :
```python
path_planning_with_cost(
      x_x=5, x_y=1, s=1, T=5, epochs=5,
      stepsize=0.01, cost_f=vanilla_cost, debug=False
)
```
