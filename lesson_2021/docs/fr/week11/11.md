---
lang: fr
lang-ref: ch.11
title: Semaine 11
translation-date: 19 June 2021
translator: Loïck Bourdois
---


<!--
## Lecture part A
We provide an introduction to the problem of speech recognition using neural models, emphasizing the CTC loss for training and inference when input and output sequences are of different lengths.
-->
## Cours magistral partie A
Nous présentons une introduction au problème de la reconnaissance de la parole à l'aide de modèles neuronaux en mettant l'accent sur la perte CTC (*Connectionist Temporal Classification*) pour l'entraînement et l'inférence lorsque les séquences d'entrée et de sortie sont de longueurs différentes.

<!--
## Lecture part B
We discuss beam search for use during inference, and how that procedure may be modeled at training time using a Graph Transformer Network. Graph transformers networks are basically weighted finite-state automata with automatic differentiation, that allows us to encode priors into a graph. There are different type of weighted finite-state and different operations including union, Kleene closure, intersection, compose, and forward score. The loss function is usually the difference between to functions. We can easily implement these networks using GTN library.
-->
## Cours magistral partie B
Nous discutons de l'utilisation de la recherche en faisceau pendant l'inférence ainsi que de la façon dont cette procédure peut être modélisée au moment de l'entraînement d'un *Graph Transformer Network* (GTN). Les GTNs sont essentiellement des « accepteur d'état fini pondéré » (WFSA pour « Weighted Finite State Acceptor ») avec différenciation automatique permettant d'encoder des a priori dans un graphe. Il existe différents types d'états finis pondérés et opérations, notamment l'union, l'étoile de Kleene, l'intersection, la composition et le score *forward*. La fonction de perte est généralement la différence entre deux fonctions. Nous pouvons facilement implémenter ces réseaux en utilisant la bibliothèque *gtn*.

<!--
## Practicum
-->
## Travaux dirigés
